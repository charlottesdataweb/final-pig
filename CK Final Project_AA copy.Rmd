---
title: "Final Project"
author: "CJK"
date: "2023-07-18"
output: html_document
cache: FALSE
---

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

ADVANCED ANALYTICS - FINAL PROJECT
This is the R file, with comments to document, for my final project in Advanced Analytics. 

#PACKAGES, CONFLICT PREFERENCES, HELPER FUNCTIONS
I will first run pacman to make sure I have downloaded all packages I should need, then I will call all the packages to my library.
```{r pacman, echo = FALSE, warning = FALSE, message = FALSE}
tryCatch(require(pacman),finally=utils:::install.packages(pkgs='pacman',repos='http://cran.r-project.org'));
require(pacman)

pacman::p_load(Hmisc,
               checkmate,
               corrr,
               conflicted,
               readxl,
               readr,
               dplyr,
               tidyr,
               ggplot2,
               knitr,
               evaluate,
               iopsych,
               psych,
               quantreg,
               lavaan,
               xtable,
               reshape2,
               GPArotation,
               Amelia,
               esquisse,
               expss,
               multilevel,
               janitor,
               mice,
               lmtest,
               curl,
               MASS,
               RCurl,
               DT,
               modelr,
               broom,
               purrr,
               pROC,
               rsample,
               tidymodels,
               tidyposterior,
               easystats,
               report,
               data.table,
               VIM,
               gridExtra,
               Metrics,
               randomForest,
               e1071,
               corrplot,
               DMwR2,
               rsample,
               skimr,
               tree,
               janitor,
               GGally,
               tidyquant,
               doParallel,
               Boruta,
               correlationfunnel,
               naniar,
               plotly,
               themis,
               questionr,
               tidylog
)
```


I will make notes of what each package accomplishes so I have this as a reference point later on.
I also consider this a starter list of packages. I may need a few more as I progress.
```{r packages, echo = FALSE, include = FALSE, warning = FALSE, message = FALSE}
suppressPackageStartupMessages({
library(Hmisc) # Contains many functions useful for data analysis
library(tidyverse) # The all powerful tidyverse metapackage
library(checkmate) # Fast and Versatile Argument Checks
library(corrr) # Correlations in R
library(conflicted) # An Alternative Conflict Resolution Strategy
library(readxl) # read in Excel files
library(readr) # read in csv files
library(MASS) # Functions and datasets to support Venables and Ripley
library(dplyr) # A Grammar of Data Manipulation
library(tidyr) # Tidy Messy Data
library(broom) # Convert Statistical Objects into Tidy Tibbles
library(ggplot2) # grammar of graphics for visualization
library(finetune) # The ability to tune models is important. 'finetune' enhances the 'tune' package by providing more specialized methods for finding reasonable values of model tuning
library(knitr) # A General-Purpose Package for Dynamic Report Generation in R
library(evaluate) # Parsing and Evaluation Tools that Provide More Details than the Default
library(iopsych) # Methods for Industrial/Organizational Psychology
library(Amelia) # Identifies missing data
library(esquisse) # Useful for data exploration and visualization
library(multilevel) # Multilevel Functions
library(janitor) # 	Simple Tools for Examining and Cleaning Dirty Data
library(mice) # Multivariate Imputation by Chained Equations
library(skimr) # Exploratory Data Analysis
library(lmtest) # A collection of tests, data sets, and examples for diagnostic checking in linear regression models 
library(RCurl) # General Network (HTTP/FTP/...) Client Interface for R
library(DT) # A Wrapper of the JavaScript Library 'DataTables'
library(modelr) # Modelling Functions that Work with the Pipe
library(purrr) # Functional Programming Tools - helps with mapping (i.e., loops)
library(pROC) #	Display and Analyze ROC Curves
library(rsample) # General Resampling Infrastructure
library(skimr) # Compact and Flexible Summaries of Data
library(psych) # Procedures for Psychological, Psychometric, and Personality Research
library(tree) # Classification and Regression Trees
library(probably) # threshold manipulation for tidymodels
library(tidymodels) # Easily Install and Load the 'Tidymodels' Packages
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(dataxray) # An interactive table interface for data summaries
library(tidyposterior) # conduct post hoc analyses of resampling results generated by models
library(easystats) # Create nice reports of statistical output including effect size
library(report) # Create nice reports of statistical output including effect size
library(data.table) # Fast aggregation of large data (e.g. 100GB in RAM)
library(VIM) # Visualization and Imputation of Missing Values
library(gridExtra) # Miscellaneous Functions for "Grid" Graphics
library(Metrics) # Evaluation Metrics for Machine Learning
library(randomForest) # Breiman and Cutler's Random Forests for Classification and Regression
library(e1071) # Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien
library(corrplot) # Visualization of a Correlation Matrix
library(DMwR2) # Functions and Data for the Second Edition of "Data Mining with R"
library(glmnet) # generalized linear model with LASSO, Ridge, and Elasticnet
library(GGally) # Extension to 'ggplot2'
library(tidyquant) # Tidy Quantitative Financial Analysis
library(doParallel) # Foreach Parallel Adaptor for the 'parallel' Package
library(Boruta) # Wrapper Algorithm for All Relevant Feature Selection
library(correlationfunnel) # Speed Up Exploratory Data Analysis (EDA) with the Correlation Funnel
library(naniar) # viewing and handling missing data
library(plotly) # Create interactive plots
library(themis) # Upsampling and Downsampling methods for tidymodels
library(questionr) # this will give you odds ratios
library(tidylog) # keeps log of what the code is doing in the background
})

for (f in getNamespaceExports("tidylog")) {
    conflicted::conflict_prefer(f, "tidylog", quiet = TRUE)
}
```

I will also set my conflict preferences. Some functions are found under multiple packages, and this tells R to use a certain function from one package over other package options.
This is a 'starter' of conflict preferences, and I will add more as the need arises.
```{r preferences}
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("select_if","dplyr")
conflict_prefer("tune", "tune")
conflict_prefer("chisq.test", "stats")
conflict_prefer("filter", "dplyr")
conflict_prefer("skewness", "PerformanceAnalytics")
conflict_prefer("fit", "parsnip")
conflict_prefer("rmse", "yardstick")
conflict_prefer("map", "purrr")
conflict_prefer("vip", "vip") 
```


These are helper function from Matt Dancho. These will help me later on as I create certain visualizations and mutate variables.
```{r help functions}
plot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {
    
    color_expr <- enquo(color)
    
    if (rlang::quo_is_null(color_expr)) {
        
        g <- data %>%
            ggpairs(lower = "blank") 
        
    } else {
        
        color_name <- quo_name(color_expr)
        
        g <- data %>%
            ggpairs(mapping = aes_string(color = color_name), 
                    lower = "blank", legend = 1,
                    diag = list(continuous = wrap("densityDiag", 
                                                  alpha = density_alpha))) +
            theme(legend.position = "bottom")
    }
    
    return(g)
    
}

#From Matt Dancho DS4B 201
plot_hist_facet <- function(data, fct_reorder = FALSE, fct_rev = FALSE, 
                            bins = 10, fill = palette_light()[[3]], color = "white", ncol = 5, scale = "free") {
    
    data_factored <- data %>%
        mutate_if(is.character, as.factor) %>%
        mutate_if(is.factor, as.numeric) %>%
        gather(key = key, value = value, factor_key = TRUE) 
    
    if (fct_reorder) {
        data_factored <- data_factored %>%
            mutate(key = as.character(key) %>% as.factor())
    }
    
    if (fct_rev) {
        data_factored <- data_factored %>%
            mutate(key = fct_rev(key))
    }
    
    g <- data_factored %>%
        ggplot(aes(x = value, group = key)) +
        geom_histogram(bins = bins, fill = fill, color = color) +
        facet_wrap(~ key, ncol = ncol, scale = scale) + 
        theme_tq()
    
    return(g)
    
}
```


#DATA UPLOAD

Now that I have my packages, conflicts, and helpers set up, the first official step is to import the dataset into R. After it uploads, I will change the name of this original set to Data for ease of use, especially since the original name of the data set is quite long.

```{r data}
PSYC6841_Final_Project_Data_1_ <- read_excel("~/Documents/DirectoryR/UGA IOMP 2023 AA/PSYC6841_Final_Project_Data (1).xlsx")
View(PSYC6841_Final_Project_Data_1_)
Data <- PSYC6841_Final_Project_Data_1_
rm(PSYC6841_Final_Project_Data_1_) #Removing since we don't need it anymore
```


#SEED SETTING

Now that I have my dataset uploaded, I will set my seed. Seed-setting helps control the randomness so results can be replicated as needed (or in other cases, seed-setting can produce two different results from the same set of data to see if any results and patterns remain consistent, but for my purposes, I am using it for reproducability).
```{r set seed}
set.seed(2023)
```


I will start with four 'getting to know you' functions. In order, I will glimpse the data, look at column names, examine the data's structure, and take a look at a summary of the data. 
```{r data intro}
glimpse(Data)
dput(colnames(Data))
str(Data)
summary(Data)
```

There are 9,999 observations (respondents), and 10 total variables, including my eventual outcome variable of attrition (which, in this dataset, is called 'left').
Of the 10 variables, 8 are coded as numerical (it does appear that some of these are categorical and binary, including my outcome variable, so I will note this for later on), and 2 are coded as characters. I will likely want to change some of these variables to factors, including character variables and my binary variables. For now though, these are just notes for my future self.

#NON-NUMERICAL VARIABLES
Next, I will look more deeply at my 2 character variables. I will look at them a few different ways.

```{r character var}
Data %>%
    select_if(is.character) %>%
    map(unique)

Data %>%
    select_if(is.character) %>%
    map(~round(prop.table(table(.x)),2))
```

For my two character variables, I can glean the following:
Salary has three levels: low, medium, and high. As I noted above, I will want to change this to a factor with the levels in the appropriate order in the future. Also, high salary only constitutes 8% of respondents. Medium (.43) and low (.49) are fairly even in proportion of responses.
Sales has ten different areas/departments: accounting, HR, IT, management, marketing, product_mng (which I'm assuming is product management), R & D, sales, support, and technical. Nothing crazy there, except I will note that sales has the highest proportion of respondents (.28) followed by technical (.18) and support (.15). Together, the top three departments make up over 60% of all respondents. Also, the label of 'sales' feels strange. This could easily be relabeled as 'department' and I feel like that would make more sense. I won't make any official changes, but I may note this in my final report.



#NUMERICAL VARIABLES
Now I will look at my numeric variables in a similar fashion.

```{r numeric var}
Data %>%
    select_if(is.numeric) %>%
    map(~ unique(.) %>% length())

Data %>%
    select_if(is.numeric) %>%
    map(~round(prop.table(table(.x)),2))
```


#Here is some info about my numeric variables:
Satisfaction level has 92 unique responses and appears to be a continuous variable. Responses range from 0.09 to 1.
Last evaluation has 65 unique responses and appears to be a continuous variable. Responses range from 0.36 to 1.
Number of projects has 6 unique responses and appears to be a discrete variable. Responses range from 2-7 with the majority of responses as 3 (.27) or 4 (.28)
Average monthly hours has 215 unique responses (this high number would usually indicate a continuous variable, but it seems to actually be coded as discrete, just with a wide variance of responses). Responses range from 96 to 310.
Time spent at the company (tenure, maybe?) has 8 unique observations, likely indicating that it's discrete. Responses range from 2-10 with a high proportion of responses for 3 (.43).
Work accident, left, and promotion in the last 5 years all have 2 unique observations. These are categorical, binary variables with yes/no corresponding to 1/0. I will note this for later on as well. Work accident has majority of 0 ('no') responses at 0.85, left has a majority of 'no' responses at 0.76, and promotion has an overwhelming majority of 'no' responses at 0.98.
Since the outcome variable (left) is imbalanced with a majority of 'no' responses, I will need to focus future efforts on accounting for this imbalance. For now, once more, this is just a thing to note for my future self.
Since I don't have a codebook, I am curious to know the question prompt for some of these variables, specifically satisfaction level, last evaluation, time spent at the company, and work accident. I will see if I can get more information on this to help me interpret the responses better.

#OUTCOME AS FACTOR
I will be examining and adjusting other variables down the line, but I do want to make sure my outcome variable of attrition ('left') is changed to a factor since this will be necessary for several of my analyses and models down the line.
I will also proceed with changing my other two binary categorical variables to factors as well. I'm mostly doing this so I don't attempt to involve these variables in any analyses in which only numerical variables should be evaluated since categorical variables interpreted as numerical can lead to inaccuracies later on. 
```{r outcome factor}
conflict_prefer('mutate','dplyr')

Data <- Data %>%
    mutate(left = as.factor(left))

Data <- Data %>%
    mutate(promotion_last_5years = as.factor(promotion_last_5years))

Data <- Data %>%
    mutate(Work_accident = as.factor(Work_accident))

```

#SALARY TO FACTOR WITH LEVELS
I will also move forward with making my salary variable into a factor with three distinct levels. Usually R automatically assigns levels alphabetically, so I will need to ensure the levels are in the correct order (low, medium, high) moving forward.
```{r salary factor}
Data <- Data %>%
    mutate(salary = as.factor(salary))

Data <- Data %>%
  mutate(salary = factor(salary,
                                 levels = c("low",
                                            "medium",
                                            "high")))
```


#CREATING ID NUMBER
I noticed that there is no ID number, or similar identification-based column. I will create that now and use ID as the method for splitting my data. I will also use the select() function to make sure ID is the first column followed by all other variables.
This ID number will be useful very soon when I split my data into a training set and a test set. I just need to remember to deselect it from certain analyses later on so R isn't interpreting it as an actual variable.
```{r ID}
Data <- Data %>% 
    mutate(ID = row_number()) %>%
    select(ID, everything())
```


#DUPLICATES
Now I will see if there are any duplicates in my rows. Duplicates are suspicious and may be due to some sort of computational/input human error, so I will need to deal with them if they are present in my data.
```{r duplicates}
sum(duplicated(Data %>% select(-ID)))
```

I have 1446 duplicates, which is not ideal. This could be a computational error (which feels likely), it could be legitimate responses that are fully identical (which feels less likely), or perhaps a combination of the two. Regardless, this is something that could be problematic for some future analyses, so I am going to remove the duplicates from my dataset. The good news is that I have a large dataset of almost 10000 observations, so losing over 1400 is not as tragic as it would be with a smaller dataset.
```{r duplicate removal ECHO = FALSE}
which(duplicated(Data %>% select(-ID)))

index <- which(duplicated((Data %>% select(-ID))))
      
Data_no_dup <- Data[-index, ]

```
      
My new dataset has 8553 observations, and this is what I will proceed with to split my data into two sets.


#DATA SPLITTING
Now that I have some understanding of what I'm working with and I have made the necessary changes to my entire dataset, I will proceed with splitting my data into a training set and a testing set. I'm mostly doing this now to ensure that I don't create any sneaky ways for the test data to get any information about the training data. For instance, if I impute missing data based on the mean of the entire dataset, or if I make any prior decisions about outliers with the whole set of data, these decisions could create data leakage, which I want to avoid at all costs! So far, nothing I have done has allowed aspects of my data to 'speak' to each other.
For my split, I will go for an 80/20 of training/testing. I mainly chose 80/20 because I want the majority of my data used for training while still retaining an adequate number of observations for my testing data. Since I have almost 10,000 observations, I am confident that 20% will still allow for enough statistical power of my testing set.
I will also use a stratified split to help account for the imbalance of data for my outcome variable 'left' - from the above exploration of numeric data, I know that the majority of participants did not leave. So I want R to take this imbalance into account as it splits the data so I end up with this approximate divide in both my training and testing sets.


```{r data split}
set.seed(2023)
data_split <- initial_split(Data_no_dup, prop = 0.80, strata = "left")

train_data <- training(data_split)

test_data <- testing(data_split)
```


This creates my 80/20 split. Now I can see if the strata function did its thing and I have approximately equal 'no' responses for 'left' in both sets of data.
```{r strata check}

tabyl(train_data$left)

tabyl(test_data$left)
```
And I do! My split is complete. Now I will focus on the training dataset only so I can ensure no conversations with the data leak into my test set.

#CREATING FOLDS FOR CV
For starters, I will go ahead and prepare for cross-validation using the v-fold cross-validation method. In a nutshell, this method divides my training data into equal parts, or 'folds' (I have chosen 10) and trained on all but one part, then validated on the remaining part, a total of 10 times. This is useful for evaluating performance of a model to decrease the chances that it is not picking up on random 'noise' and using this to overfit to my training data. V-fold CV is especially useful for imbalanced datasets, which I have. Once more, I will use the strata function to ensure each fold is taking the responses to 'left' into account and dividing observations accordingly. I also will not use this immediately, but I wanted to tackle it while I was already doing the other splitting-type functions.

```{r v folds}
cv_folds <- vfold_cv(train_data, v = 10, strata = "left")

```


#EDA (Exploratory Data Analysis)

Now I can proceed with further exploratory data analysis, including checking for anything problematic I will need to handle with my data.

#MISSINGNESS
For starters, I will check to see if there is any missing data. There are a few methods to do this, but for now, I will use a missingness map from the Amelia package and double check it with the table from the nania package.
For fun, I will use UGA colors, red and black, for my missingness map.
```{r missing data}
missmap(train_data, y.at=c(1), y.labels=c(''), col=c('red', 'black'))
gg_miss_var(train_data)
```


Great news, it looks like there is no missing data! This is a rare treat, so I will enjoy not needing to handle any missingness - several of the models I can use do not handle missing data well!

#OUTLIERS
Next up, I'm going to evaluate the dataset for multivariate and univariate outliers to create a plan for handling any unusual responses and response patterns.

MULTIVARIATE OUTLIERS
Now I'm going to check for unusual responses/outliers using the Mahalanobis distance. This looks for observations that are unusually far away from the majority of other observations. I will then examine the outliers and decide on a game plan for handling them. If they appear to be unusual responses that are still valid and seem accurate, I will likely keep them in. If they appear to correspond with inaccurate or careless responses, I will probably choose to remove them.
I do need to make two adjustments to the data I use to examine outliers. One, I want to make sure that I am not having the analysis evaluate the ID number I created, and two, I need to exclude the factor variables since only numeric variables can be used to calculate the Mahalanobis distance. I will use this first line of data to create a new dataframe that only includes my 5 numerical variables.
```{r mahal}
mahal_train <- train_data[,2:6]
cutoff = qchisq(1-.001, ncol(mahal_train))
mahal = mahalanobis(mahal_train,
                    colMeans(mahal_train),
                    cov(mahal_train))
cutoff
ncol(mahal_train)
summary(mahal < cutoff)
```

According to the Mahalanobis results, I have 87 potential outliers. I'm going to make the Mahalanobis value its own column and bind it to my new dataframe.
```{r outliers}
mahal_train <- mahal_train %>%
    bind_cols(mahal) %>%
    tidylog::rename(mahal = `...6`) 
```

And I'm going to create a dataframe of only the 89 potential outliers to take a deeper look into the responses. I am looking for careless, unusual, or illogical responses that may indicate inaccurate observations that would erroneously affect the results of certain statistical analyses and modeling.
```{r outliers multi}
mahal_out <- mahal_train %>%
    filter(mahal > cutoff) %>%
    arrange(desc(mahal))

mahal_out
```

The first insight I gleaned from the outliers is the large quantity of responses of '10' for time spent at the company. I wonder if this was the primary difference between potential outlier responses and other responses not flagged as outliers. To further evaluate this, I looked at the entire dataset for mahal_train and did find that almost all '10' responses had significantly higher Mahalanobis distance values. I feared that removing these responses would remove some likely useful variance in the responses to the 'time spent at the the company' question.

This analysis also did not include any of my binary variables, since Mahalanobis distance is not recommended for categorical and factor variables. This is especially noteworthy since my outcome variable (left) is one of the three factor variables. Since this analysis is not able to take into account the responses for the outcome variable, I do not feel comfortable removing these responses solely based on the numeric data alone.

Lastly, when I dug deeper into the responses for potential outliers, nothing seemed inaccurate, careless, or very out of place. All responses seemed logical and within normal limits based on my evaluations thus far of the overall dataset. Because of the logical and seemingly accurate responses, and because I feel that there could be highly valuable information in retaining a more comprehensive picture of responses to the 'time spent at the company'  question, I feel that the best decision moving forward is to retain the respondents flagged as potential multivariate outliers.


UNIVARIATE OUTLIERS
Now, I will take a look at univariate outliers as well. I am using this for two main reasons: it may identify an unusual response not identified by the Mahalanobis that may be due to careless responses or computational errors; and it can also help me double check what I gleaned from my analysis of the potential multivariate outliers (if a respondent appears in the following code for univariate outlier identification AND in the above code for multivariate outliers, I likely missed something fishy that I need to handle).

I will do this by creating a z-score threshold to see if any responses for my nominal variables are +/- 3 standard deviations away from the mean.
First, I will create a new dataset in which I standardize my nominal variables
```{r outliers uni}
standardized_data <- train_data %>%
    mutate(across(all_nominal(), scale))
#Then I will set my z-score threshold.

z_score_threshold <- 3

#And lastly, I will use this threshold to pull all univariate outliers to a new data frame.
outliers <- standardized_data %>%
    dplyr::filter_if(is.numeric, ~. > z_score_threshold | . < -z_score_threshold)
```

Great news, there are no identified univariate outliers! I can now feel even more secure in my decision to retain all multivariate outliers.


#ASSUMPTION CHECKS
Next, I will perform the necessary assumption checks that apply to some of the potential models I will use to fit the data on later.
It is worth noting that this is not a 'one size fits all' regarding assumption checks - meaning that some of these assumptions only apply to some of the potential models I could use but not others, so failing to meet certain assumptions is not a dealbreaker for my data across the board.

ADDITIVITY / COLLINEARITY
This process will start with examining the data for additivity and collinearity. This means that I don't want my predictor variables to be so highly correlated with each other that their contributions are no longer independent of one another. In other words, I don't want high collinearity, and I want my variables to have an additive influence on my outcome variable.
Similar to my outliers check, I will deselect my non-numerical variables and only examine numerical variables. What I am looking for is small correlations between variables.

```{r additivity}
correl = cor(train_data %>% select(-ID,-salary,-sales,-left,-promotion_last_5years,-Work_accident), use = "pairwise.complete.obs")
#note that I removed all three binary variables, even though I have not yet changed promotion and accident to be factors. however, they should still not be treated as numeric variables.
symnum(correl)

correl
```
And it looks like I'm in good shape. The highest correlations between variables are at roughly .30 to .36. This means my variables have an additive relationship to one another and aren't too related to create collinearity issues later on.


#DATA VISUALIZATION - TABLES AND GRAPHS
Data visualization is useful for noting any other unusual or potentially problematic trends in the data. I will be examining tables and graphs for all of my variables individually, and I will also create some visual representations of my outcome variable paired with a selection of my predictor variables. I will use my previous explorations of the data, as well as my examinations of their individual tables/graphs, to determine which predictor variables would provide valuable insights paired with the outcome variable of attrition ('left').

I will take each variable and create both a table and a graph for it. I have 10 total variables, so I should have 10 tables and 10 graphs by the end. I will number each variable as I go to make sure I've accounted for all of them.

```{r preferences contd}
conflict_prefer('group_by','dplyr')
conflict_prefer('summarise','dplyr')
conflict_prefer('tally','dplyr')
```

#Discrete Numerical Variables

#1 Table and Bar Graph of 'Number of Projects'
```{r table graph 1}
train_data %>%
    tabyl(number_project) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

ggplot(train_data, mapping = aes(x=number_project)) +
    geom_bar(fill="lightblue", color="darkblue",position = 'identity',width = 1)+ 
    ylim(0,2250) + xlim(1,8) +
    labs(title="Frequency of Responses for Number of Projects",x="Number of Projects", y = "Frequency of Responses")+
    theme_classic()
```

#2 Table and Bar Graph of 'Time Spent at Company'
```{r table graph 2}
train_data %>%
    tabyl(time_spend_company) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

time_levels <- as.factor(c(2:10))

ggplot(train_data, aes(x = factor(time_spend_company))) +
    geom_bar(fill = "lightblue", color = "darkblue") +
    scale_x_discrete(limits = time_levels) +
    ylim(0, 3500) +
    labs(title = "Frequency of Responses for Time Spent at Company", x = "Time Spent at Company", y = "Frequency of Responses") +
    theme_classic()
```

#3 Histogram of 'Average Monthly Hours'
For the table, I am not using tabyl anymore because of how many different values there are. I will use instead use dplyr and the bin function to separate the data into smaller ranges by 20 (90-110,110-130, etc). I also chose a histogram here because, even though this is a discrete variable, there are so many values that a histogram captures the overall pattern better than a binned bar graph (which also labeled things strangely, so all the more reason to go with a histogram!)
Essentially, even though this is a discrete variable, the variance and number of different possible values lead me to treat it more as a continuous variable (as you will see with the similarities of my treatment of the continuous variables below for variables #9 and #10)

```{r table graph 3}
train_data %>%
        mutate(bins = cut(average_monthly_hours, breaks = seq(90, 310, by = 20), include.lowest = TRUE)) %>%
        group_by(bins) %>%
        summarise(Frequency = n(), Percentage = n() / nrow(train_data) * 100)

ggplot(train_data, aes(x = average_monthly_hours)) +
    geom_histogram(fill = "lightblue", color = "darkblue", position = 'identity',width = 1,bins = 20) + ylim(0,700) +
    labs(title = "Frequency of Responses for Average Monthly Hours",
         x = "Average Monthly Hours",
         y = "Frequency of Responses") +
    theme_classic()
```

#Categorical Factor Variables
#4 Table and Bar Graph of 'Sales' (which I am interpreting as 'Departments')
I adjusted the labels to make sure everything is capitalized and no weird underscores or abbreviations were visible for each bar
```{r table graph 4}
train_data %>%
    tabyl(sales) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

ggplot(train_data, mapping = aes(x=sales)) +
    geom_bar(fill="lightblue", color="darkblue",position = 'identity',width = 1)+ 
    ylim(0,2000) +
    labs(title="Frequency of Responses for  Department Types",x="Department Type", y = "Frequency of Responses")+
    theme_classic() +
    scale_x_discrete(labels = c("Accounting", "HR", "IT","Management","Marketing","Product Mgmt",'R & D','Sales','Support','Technical'))
```

#5 Table and Bar Graph of 'Salary'
I adjusted the labels to capitalize the three different salary levels
```{r table graph 5}
train_data %>%
    tabyl(salary) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

ggplot(train_data, mapping = aes(x=salary)) +
    geom_bar(fill="lightblue", color="darkblue",position = 'identity',width = 1)+ 
    ylim(0,3500) +
    labs(title="Frequency of Responses for Salary Levels",x="Salary Level", y = "Frequency of Responses")+
    theme_classic() + 
    scale_x_discrete(labels = c("Low", "Medium", "High"))
```

#Binary Variables (Yes/No)
#6 Table and Bar Graph of 'Promotion in Last Five Years'
```{r table graph 6}
train_data %>%
    tabyl(promotion_last_5years) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

train_data %>%
    mutate(promotion_last_5years = factor(promotion_last_5years, labels = c("No", "Yes"))) %>%
    group_by(promotion_last_5years) %>%
    tally() %>%
    ggplot(aes(x = promotion_last_5years, y = n, fill = promotion_last_5years)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(x = "Promotion in Last Five Years", y = "Count of Promotion in Last Five Years") +
    ggtitle("Frequency of Responses for Promotion in Last Five Years") +
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```

#7 Table and Bar Graph of 'Work Accident'
```{r table graph 7}
train_data %>%
    tabyl(Work_accident) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

train_data %>%
    mutate(Work_accident = factor(Work_accident, labels = c("No", "Yes"))) %>%
    group_by(Work_accident) %>%
    tally() %>%
    ggplot(aes(x = Work_accident, y = n,fill=Work_accident)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(x="Work Accident", y="Count of Work Accidents") +
    ggtitle("Frequency of Responses for Occurrence of Work Accidents") +
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```

#8 Bar Graph of 'Left' (attrition), OUTCOME VARIABLE
```{r table graph 8}
train_data %>%
    tabyl(left) %>% 
    adorn_pct_formatting(digits = 0, affix_sign = TRUE)

train_data %>%
    mutate(left = factor(left, labels = c("No", "Yes"))) %>%
    group_by(left) %>%
    tally() %>%
    ggplot(aes(x = left, y = n,fill=left)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(x="Attrition ('left')", y="Count of Attrition ('left'") +
    ggtitle("Frequency of Responses for Occurrence of Attrition ('left')") +
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```

#Continuous Variables
#9 Histogram of 'Satisfaction Level'
Similar to average monthly hours, I am using a bin strategy for my table. The frequency of the individual values is not helpful to look at. This instead bins the values into small ranges (0-.1, .1-.2, etc)
```{r table graph 9}
train_data %>%
    mutate(bins = cut(satisfaction_level, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %>%
    group_by(bins) %>%
    summarise(Frequency = n(), Percentage = n() / nrow(train_data) * 100)

ggplot(train_data, aes(x = satisfaction_level)) +
    geom_histogram(fill = "lightblue", color = "darkblue", position = 'identity',width = 1,bins = 20) +ylim(0,600) +
    labs(title = "Frequency of Responses for Satisfaction Level",
         x = "Satisfaction Level (on a scale from 0-1)",
         y = "Frequency of Responses") +
    theme_classic()
```

#10 Histogram of 'Last Evaluation'
I used the same table strategy from satisfaction level here, using bins to group the data into small ranges .1 apart from each other.
```{r table graph 10}
train_data %>%
    mutate(bins = cut(last_evaluation, breaks = seq(.3, 1, by = 0.1), include.lowest = TRUE)) %>%
    group_by(bins) %>%
    summarise(Frequency = n(), Percentage = n() / nrow(train_data) * 100)

ggplot(train_data, aes(x = last_evaluation)) +
    geom_histogram(fill = "lightblue", color = "darkblue", position = 'identity',width = 1,bins = 20) + ylim(0,600) +
    labs(title = "Frequency of Responses for Last Evaluation",
         x = "Last Evaluation (on a scale of 0-1)",
         y = "Frequency of Responses") +
    theme_classic()
```


#DATA VISUALIZATION CONTINUED - CROSSTABS OF VARIABLES
Next, I will look at some crosstabs of my variables to see the relationships between/among different variables. I will likely involve my outcome variable (attrition/'left') in most or all of these crosstabs, since it is the variable of interest for this project


#Attrition by Salary
```{r attrition salary table}
train_data %>%
  group_by(salary) %>%
  summarise(avg_attrition = round(mean(left == "1"), 2)) %>%
  arrange(desc(avg_attrition))

```
It does look like attrition rates are highest for low salary employees (0.23) and lowest for high salary employees (0.06). Medium salary attrition rates are between high and low (0.16). This makes intuitive sense. The more money you make, the more likely you are to remain with an organization.
Let's look at a graphical visualization of these two variables

```{r attrition salary graph}
train_data %>%
    mutate(left = factor(left, labels = c("No", "Yes"))) %>%
    group_by(salary, left) %>%
    tally() %>%
    ggplot(aes(x = salary, y = n,fill=left)) +
    geom_bar(stat = "identity") +
    theme_minimal()+
    labs(x="Salary Level", y="Attrition (Yes/No)")+
    ggtitle("Attrition by Salary Level")+
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```
This stacked bar chart shows the same pattern seen in the averages table above - low salary has the highest rate and percentage of attrition, followed by medium, then high.


#Attrition by Satisfaction Level
Next I will explore the relationship between satisfaction level and attrition. Are less satisfied employees more likely to leave the organization?
Because sat level is continuous with so many possible values, I'm going to continue looking at it through binned ranges of values so it is more straightforward to interpret and visualize. Let's start with a table of the average attrition rates by satisfaction level ranges broken up by 0.1
```{r attrition sat table}
train_data %>%
    mutate(bins = cut(satisfaction_level, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %>%
    group_by(bins) %>%
    summarise(Average_Attrition = mean(as.numeric(left == '1')),
              Frequency = n(),
              Percentage = n() / nrow(train_data) * 100)
```
This is very informative. There is a definite trend of less satisfied employees being more likely to leave, but there is also an interesting pattern within the 0.5-1 range - those with sat levels 0.5-0.7 are less likely to leave than those with 0.7-0.9. This is noteworthy as I move forward.
Also, it's weird how low attrition rates are for those with 0.20-0.30 sat levels, but maybe that's an anomaly (especially since there weren't many observations in that range anyways).
Let's see if all of this is reflected in the graph as well.

```{r attrition sat graph}
train_data %>%
    mutate(left = factor(left, labels = c("No", "Yes")),
           satisfaction_bin = cut(satisfaction_level, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1),
                                  labels = c("0-0.1", "0.1-0.2", "0.2-0.3", "0.3-0.4", "0.4-0.5", "0.5-0.6", "0.6-0.7", "0.7-0.8", "0.8-0.9", "0.9-1.0"),
                                  include.lowest = TRUE)) %>%
    group_by(satisfaction_bin, left) %>%
    tally() %>%
    ggplot(aes(x = satisfaction_bin, y = n, fill = left)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(x = "Satisfaction Level", y = "Attrition (Yes/No)") +
    ggtitle("Attrition by Satisfaction Level") +
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```
These same patterns are present in the stacked bar chart as well. I will be interested to see how this relationship plays out as the analyses continue.


#Attrition by Average Monthly Hours
Next I will look at how the number of hours worked each month influences attrition levels. I would assume to see a pattern of more hours being more likely to lead to attrition (more hours = more stress and likelihood of burnout), but I will see if that's reflected in the data
Once more, I will look at bins of the data here since there are so many values for my monthly hours variable.
```{r attrition hours table}
train_data %>%
    mutate(bins = cut(average_monthly_hours, breaks = seq(90, 310, by = 20), include.lowest = TRUE)) %>%
    group_by(bins) %>%
    summarise(Average_Attrition = mean(as.numeric(left == '1')),
              Frequency = n(),
              Percentage = n() / nrow(train_data) * 100)
```
Interestingly from my table, it looks like those with the lowest and the highest hours are more likely to leave. This could make sense, if people are feeling disengaged enough to cut back on work - or if they are not being well-utilized so their hours got cut by someone else, this may motivate them to leave. Let's visualize the data as well.


```{r attrition hours graph}
#Here is a boxplot first
ggplot(train_data) +
    geom_boxplot(data = subset(train_data, left == 0),
                 aes(x = average_monthly_hours, y = left),
                 fill = "lightblue", color = "darkblue") +
    geom_boxplot(data = subset(train_data, left == 1),
                 aes(x = average_monthly_hours, y = left),
                 fill = "lightgreen", color = "darkgreen") +
    labs(title = "Box Plot of Average Monthly Hours by Attrition",
         x = "Average Monthly Hours",
         y = "Attrition") +
    theme_classic() +
    scale_y_discrete(breaks = c(0, 1), labels = c("No", "Yes"))
#Weirdly the 'yes' and 'no' on the y-axis are no longer showing up. Possibly from me changing the variable to a factor early on.

#And here is a stacked bar chart
train_data %>%
    mutate(left = factor(left, labels = c("No", "Yes")),
           monthlyhours_bin = cut(average_monthly_hours, breaks = c(90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310),
                                  labels = c("90-110", "110-130", "130-150", "150-170", "170-190", "190-210", "210-230", "230-250", "250-270", "270-290", "290-310"),
                                  include.lowest = TRUE)) %>%
    group_by(monthlyhours_bin, left) %>%
    tally() %>%
    ggplot(aes(x = monthlyhours_bin, y = n, fill = left)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    labs(x = "Average Monthly Hours ", y = "Attrition (Yes/No)") +
    ggtitle("Attrition by Average Monthly Hours") +
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```
The box plot makes it seem like there is no real relationship between hours and attrition. The stacked bar chart shows the nuance of the relationship a little better. But it is surpsising that the pattern indicates those most likely to quit are those working the least number of hours.


#Attrition by Department
And I will look at attrition levels by department (the variable of 'sales'). I don't currently have a working hypothesis about which departments may be 'in trouble' - first I will look at the average attrition for each department
```{r attrition dept table}
train_data %>%
    group_by(sales) %>% 
    summarise(avg_attrition = round(mean(as.numeric(left == '1')),2)) %>%
    arrange(desc(avg_attrition))
```
There does not appear to be any major trend here. Attrition rates are relatively balanced across most departments. Some are a bit higher and others are a bit lower, but they're all within .06 of each other. This is useful information - it seems that department/job type may not be a major factor in determining attrition rates. Next I will look at a stacked bar chart.
```{r attrition dept graph}
train_data %>%
    mutate(left = factor(left, labels = c("No", "Yes"))) %>%
    group_by(sales, left) %>%
    tally() %>%
    ggplot(aes(x = sales, y = n,fill=left)) +
    geom_bar(stat = "identity") +
    theme_minimal()+
    labs(x="Department/Sales", y="Attrition (Yes/No)")+
    ggtitle("Attrition by Department/Sales")+
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))
```
Sales has the highest number of attrition, but it also has the highest number of employees overall so this makes sense. Proportionately, it does seem to be pretty evenly dispersed across all departments. This makes me wonder if this variable will be removed in feature selection later down the road.


Now just for fun, I'm going to look at two last visual representations of the data. The first is salary by time spent at the company, and the second is the relationship among three variables: sat level, monthly hours, and attrition.
I'm curious to know if any new patterns will emerge from this.

#Salary by Time (TENURE)
```{r salary time}
#Stacked bar chart
train_data %>%
    group_by(time_spend_company, salary) %>%
    tally() %>%
    ggplot(aes(x = time_spend_company, y = n,fill=salary)) +
    geom_bar(stat = "identity") +
    theme_classic() +
    labs(x="Time Spent at Company", y="Salary Level (Low/Med/High")+
    ggtitle("Salary Level by Time Spent at Company")+
    geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9))

#That was a little crowded, here's three bar charts, one for each salary level
train_data %>%
  group_by(time_spend_company, salary) %>%
  tally() %>%
  ggplot(aes(x = time_spend_company, y = n, fill = salary)) +
  geom_bar(stat = "identity") +
  theme_classic() +
  labs(x = "Time Spent at Company", y = "Count") +
  ggtitle("Salary Level by Time Spent at Company") +
  geom_text(aes(label = n), vjust = -0.5, position = position_dodge(0.9)) +
  facet_wrap(~salary, ncol = 1)

```
Things look relatively evenly distributed - I would have thought that, as time spent at company increases, salary level would also increase. However, I also don't have a codebook so I don't have a full interpretation of what the levels for time spent at company mean - is 10 = 10 years? Based on the info I do have access to, nothing seems super out of place here, other than the relative lack of differences among salary levels.

#Attrition by Sat Level and Hours
Lastly, I did one three-way visualization of the relationship among satisfaction level, hours, and attrition. 
I commented out the first code trying to examine a table of this relationship, but the table was so long and virtually impossible to interpret. The code for the graph is much easier to understand.
```{r attrition sat hours}
# train_data %>%
#     tabyl(average_monthly_hours, satisfaction_level, left) %>%
#     adorn_totals("row") %>%
#     adorn_percentages("col")

ggplot(train_data, aes(x = satisfaction_level, y = average_monthly_hours, color = factor(left))) +
    geom_point() +
    labs(title = "Scatter Plot of Satisfaction Level and Average Monthly Hours",
         x = "Satisfaction Level",
         y = "Average Monthly Hours",
         color = "Attrition") +
    theme_classic()
```
Very interesting! Unsurprisingly, there is a pocket of individuals with very low satisfaction levels and very high monthly hours who leave the organization. There is also a pocket of low-ish (roughly .35-.45) sat levels and lower monthly hours who leave as well. And the last pattern is the most noteworthy to me: there is a cluster of individuals with a higher rate of attrition (not as saturated as the other two areas) who have relatively high sat levels and medium to high monthly hours. This is intriguing, and I will be curious to see how these relationships continue to play out in further analyses.



I have completed my EDA, data visualization, and examination of foundational information for the relationships between and among some of the variables.
Overall, the data visualization process has made me wonder if some of my variables will be deemed unimportant during feature selection. At least from the variables I have chosen, I haven't yet identified one or two variables that I think could be major players in determining the variance of yes and no responses for attrition in my data set. 

I will now move on to the next aspect of the analysis, which is building out different models I can use to explain and predict attrition.

#PREPROCESSING
Here is the list from Rob Stilson (credit!) for preprocessing steps used in creating a recipe

* 1. Impute
* 2. Handle factor levels 
* 2B FEATURE SELECTION (I will do feature selection through Boruta here. I'm including this since it will time out well after adjusting my variables to factors)
* 3. Individual transformations for skewness and other issues
* 4. Discretize (if needed and if you have no other choice)
* 5. Create dummy variables
* 6. Create interactions
* 7. Normalization steps (center, scale, range, etc)
* 8. Multivariate transformation (e.g. PCA, spatial sign, etc)


#IMPUTE
No imputation is needed since I don't have any missing data, so I'm good to go there.


#FACTOR LEVELS
Lastly, I am adding the code below to ensure I have a way for my salary variable to be read in as 0 = low, 1 = med, 2 =  high. I will include a mutate step in my recipes to account for this.
```{r}
factor_names <- "salary"
Levels <- c("0", "1", "2") 

```


#FEATURE SELECTION - CORRELATION FUNNEL & BORUTA
Next up, I will make a slight detour from the preprocessing journey to do feature selection. I waited until this point since now I have changed the necessary variables to factors, specifically my outcome variable. I will start by looking at a correlation funnel to get a better idea of feature importance - do I need all 9 of my predictor variables, or are only a handful really explaining the variance in attrition while the rest are not?
```{r corr funnel}
hr_data_tbl <- train_data %>%
    tidyr::drop_na()

conflict_prefer('correlate','correlationfunnel')

hr_corr_tbl <- hr_data_tbl %>%
    select(-ID) %>%
    binarize(n_bins = 5, 
             thresh_infreq = 0.01, 
             name_infreq = "OTHER", 
             one_hot = TRUE) %>%
    correlate(left__1)

hr_corr_tbl %>%
    plot_correlation_funnel() %>%
    ggplotly()

```


My correlation funnel shows me that sales, promotion in 5 years, and potentially salary may not be explaining much of the variance of attrition. However, satisfaction level, number of projects, and time spent at company do seem to be explaining a good deal of the variance of attrition. The other three variables are right down the middle (average monthly hours, last evaluation, and work accident).


I will now run Boruta to help me with feature selection for my models. Boruta is useful for feature selection because it compares features against a randomized version of themselves. This can help identify what features are pulling their weight and what variables could be removed from my final models.
```{r boruta}
conflict_prefer('mutate_if','dplyr')

boruta_df <- train_data %>%
    select(-ID) %>%
    mutate_if(is.character, as.factor)

boruta_train <- Boruta(left~., data = boruta_df, doTrace = 2)

print(boruta_train)
```
Boruta deemed 8 features to be important and one to be tentative (sales). Remember with data visualization that virtually all departments had a balanced percentage of attrition. So it's unsurprising that this is the result.

I can now run TentativeRoughFix to help Boruta determine if sales should be officially deemed unimportant
```{r boruta 2}
final_boruta <- TentativeRoughFix(boruta_train)

print(final_boruta)
```
And Boruta deemed sales to be an unimportant variable. Running the code below should return with all other predictor variables except sales
```{r boruta 3}
cat(getSelectedAttributes(final_boruta, withTentative = F), sep = "\n")

```

Now I will return to my preprocessing, but I will note the results of my feature selection when I create my recipes in the future.

#ZERO VARIANCE VARIABLES
I do not have any zero variance variables (this would be a variable in which the responses are identical across all observations - meaning there is no variance for that variable)
However, it also won't hurt to include a step_nzv(all_predictors()) in my recipes, so I will probably do so just to cover all my bases. Also, since this is near zero variance and not zero variance, it will deselect variables with little to no variance as well as ones with truly no variance. I believe the only possible variable with near zero variance would be sales, which feature selection already removed, but it won't hurt to include this step in my recipes.


#TRANSFORMATIONS
I will now see if any of my numeric variables require transformations. If the data is highly skewed or experiencing heteroscedasticity, transformations can help the variables achieve a more normal distribution and balanced variance throughout.
I will first look at the degree of skewness for all numeric variables. I am looking for a dropoff point in which the degree of skewness is no longer similar to the degree of skewness for the next variable.


```{r skew}
train_data %>%
    select_if(is.numeric) %>%
    select(-ID) %>%
    map_df(skewness) %>%
    rsample::gather(factor_key = TRUE) %>%
    arrange(desc(value))
```

I have two major dropoff points in the data - one is between time spent at company (1.853) and the absolute value of sat level (.526), and the other is between number of projects (.347) and average monthly hours (0.021). I am thinking for now to set a threshold based on the first dropoff, since the difference in this dropoff is much greater in absolute value than the difference present in the next dropoff . So below, I will set my filter for values above 1.84
```{r skew dropoff}
conflict_prefer('gather','rsample')
conflict_prefer('mutate_if','dplyr')


train_data %>%
    select_if(is.numeric) %>%
    map_df(skewness) %>%
    gather(factor_key = TRUE) %>%
    arrange(desc(value)) %>%
    filter(value >= 1.84) %>%
    pull(key) %>%
    as.character()

skewed_feature_names <- train_data %>%
    select_if(is.numeric) %>%
    map_df(skewness) %>%
    gather(factor_key = TRUE) %>%
    arrange(desc(value)) %>%
    filter(value >= 1.84) %>% 
    pull(key) %>%
    as.character()
```

Now I will visualize the data for my skewed variable to determine if it has a visible skew or if something else is going on that would require my attention.
```{r skew plot}
train_data %>%
    select(all_of(skewed_feature_names)) %>%
    plot_hist_facet()
```

And this variable is indeed skewed. When I am creating my recipes later on, I can either add a step of step_YeoJohnson(skewed_feature_names) OR step_YeoJohnson(time_spend_company) - I may choose the 'skewed_feature_names' option just in case I decide to adjust the dropoff point and include other features in the transformation as well.

#DISCRETIZE
Right now, I do not believe I will need to discretize any of my variables, but I will keep my eye on satisfaction level and last evaluation since they are my two continuous variables and they do seem to have quite a lot of variance in the responses. For now, I will move forward to the next step.

#CREATE DUMMY VARIABLES
This one should be fairly straightforward. I will just make sure all my recipes include the step of step_dummy(all_nominal(), -all_outcomes()) to ensure the dummy coding of my two binary predictor variables while not including my outcome variable.

#CREATE INTERACTIONS
At this time, I am not planning to include any interaction variables. I have not seen any evidence of interactions thus far, and interactions could affect the interpretability of my results. I will proceed without interactions but I may shift this thinking later down the line.

#NORMALIZATION STEPS
I need to determine if my numeric variables require scaling and centering. Scaling and centering can help 'level the playing field' for numeric variables operating on very different scales so one variable is not deemed more important simply because the scale, range, and/or values are larger than the others. First I will take a look at all of my numeric variables.
```{r numeric scales}
train_data %>%
    select_if(is.numeric) %>%
    plot_hist_facet()
```
Some of the scales are quite different. We have sat level and evaluation as continuous variables from 0-1, number of projects and time spent at the company as discrete variables from roughly 2 to 10. Then we have the wild card of average monthly hours. It definitely has the largest values and the broadest range (about 90-310). So the move is to center and scale my numeric variables. I will do this with step_center and step_scale for all numeric variables.

#IMBALANCED DATASETS
And gathering information for my preprocessing steps is now complete. Before I start building my recipes, I do have one last factor to contend with: imbalanced datasets.
The number of 0 (no) response for attrition is so much higher than the number of 1 (yes) responses. This imbalance in my outcome variable can lead to major issues when building predictive models: mainly, the models can grow more biased to the majority class (leaning more heavily on predicting people to not leave than is warranted). I don't want that, so I will be creating a few recipes with different methods for combating imbalanced datasets: upsampling, downsampling, adysyn, and smote). I will also include a recipe with no steps to counter the imbalance and treat this as my 'baseline' recipe.
The steps for countering imbalances will also dictate how I name each recipe.

#RECIPE CREATION
The time has come to build my recipes and ensure they include all the preprocessing steps I mapped out with the information gleaned above.

First things first, I am going to use my Boruta results to only select my important features (everything but sales)
```{r feature selection vars}
train_data <- train_data %>%
    select(ID,
           satisfaction_level,
           last_evaluation,
           number_project,
           average_monthly_hours,
           time_spend_company,
           Work_accident,
           left,
           promotion_last_5years,
           salary)
#I also could have just deselected sales and it would have had the same outcome, but typing all remaining variables out as they are written in the dataframe may be helpful to me in the future.
```


Now I will build 5 recipes: a baseline (no sampling) recipe, upsampling recipe, downsampling recipe, smote recipe, and adasyn recipe.

#RECIPE 1: NO SAMPLING/BASELINE
```{r no samp rec}
library(forcats)
set.seed(2023)
no_sample_rec <- recipe(left ~ ., data = train_data) %>% #this is my baseline recipe, telling it that 'left' is my outcome and all other variables are predictors
    update_role(ID, new_role = "ID") %>% #this is making sure ID is treated as an identification number and not as a predictor variable
    # step_num2factor(factor_names,
    #                 transform = function(x) x + 1,
    #                 levels = Levels) %>%
    step_YeoJohnson(time_spend_company) %>% #this is the transformation step for my skewed variable
    step_normalize(all_numeric()) %>% #This may be redundant with the center and scaling processes below
    step_nzv(all_predictors()) %>% #This will remove predictor variables with variance near zero
    step_dummy(all_nominal_predictors()) #This will dummy code my nominal predictor variables
    
    
no_sample_rec
```

#RECIPE 2: UPSAMPLING
Upsampling will counter imbalances by increasing the number of instances of '1' (yes) responses for my outcome variable of attrition. I will set my seed first.
```{r upsample rec}
set.seed(2023)
#Most of the code will look familiar, but with the addition of the upsample step.
upsample_rec <- recipe(left ~ ., data = train_data) %>% #this is my baseline recipe, telling it that 'left' is my outcome and all other variables are predictors
    update_role(ID, new_role = "ID") %>% #this is making sure ID is treated as an identification number and not as a predictor variable
    # step_num2factor(factor_names,
    #                 transform = function(x) x + 1,
    #                 levels = Levels) %>%
    step_YeoJohnson(time_spend_company) %>% #this is the transformation step for my skewed variable
    step_normalize(all_numeric()) %>% #This may be redundant with the center and scaling processes below
    step_nzv(all_predictors()) %>% #This will remove predictor variables with variance near zero
    step_dummy(all_nominal_predictors()) %>% #This will dummy code my nominal predictor variables
    step_upsample(all_outcomes(), skip = TRUE) #This is the new upsampling step (skip = TRUE will make sure when the recipe is 'baked' that this upsampling will not apply to the test data)

upsample_rec

```

#RECIPE 3: DOWNSAMPLING
Downsampling will counter the imbalance by decreasing the number of instances of '0' (no) responses for my outcome variable.
```{r downsample rec}
set.seed(2023)
#Most of the code will look familiar, but with the downsampling step included.
downsample_rec <- recipe(left ~ ., data = train_data) %>% #this is my baseline recipe, telling it that 'left' is my outcome and all other variables are predictors
    update_role(ID, new_role = "ID") %>% #this is making sure ID is treated as an identification number and not as a predictor variable
    # step_num2factor(factor_names,
    #                 transform = function(x) x + 1,
    #                 levels = Levels) %>%
    step_YeoJohnson(time_spend_company) %>% #this is the transformation step for my skewed variable
    step_normalize(all_numeric()) %>% #This may be redundant with the center and scaling processes below
    step_nzv(all_predictors()) %>% #This will remove predictor variables with variance near zero
    step_dummy(all_nominal_predictors()) %>% #This will dummy code my nominal predictor variables
    step_downsample(all_outcomes(), skip = TRUE) #This is the new downsampling step (skip = TRUE will make sure when the recipe is 'baked' that this downsampling will not apply to the test data)

downsample_rec

```

#RECIPE 4: SMOTE
SMOTE will counter imbalances by creating fake '1' (yes) responses for my outcome variable
```{r smote rec, eval = FALSE}
# COULD NOT GET SMOTE TO WORK! I'm leaving this here to tinker with it later though
set.seed(2023)
#Most of the code will look familiar, but with the SMOTE step included.
smote_rec <- recipe(left ~ ., data = train_data) %>% #this is my baseline recipe, telling it that 'left' is my outcome and all other variables are predictors
    update_role(ID, new_role = "ID") %>% #this is making sure ID is treated as an identification number and not as a predictor variable
    step_num2factor(factor_names,
                    transform = function(x) x + 1,
                    levels = Levels) %>%
    step_YeoJohnson(skewed_feature_names) %>% #this is the transformation step for my skewed variable
    step_normalize(all_numeric()) %>%
    step_nzv(all_predictors()) %>% #This will remove predictor variables with variance near zero
    step_dummy(all_nominal_predictors()) %>% #This will dummy code my nominal predictor variables
    step_smote(all_outcomes(), skip = TRUE) #This is the new smote step (skip = TRUE will make sure when the recipe is 'baked' that the smote will not apply to the test data)

smote_rec
```

#RECIPE 5: ADASYN
Adasyn is similar to SMOTE but it creates synthetic minority responses more 'thoughtfully' by taking the values and distribution found within the data more into account in the creation of these '1' (yes) responses.
```{r adasyn rec}
set.seed(2023)
#Most of the code will look familiar, but with the downsampling step included.
adasyn_rec <- recipe(left ~ ., data = train_data) %>% #this is my baseline recipe, telling it that 'left' is my outcome and all other variables are predictors
    update_role(ID, new_role = "ID") %>% #this is making sure ID is treated as an identification number and not as a predictor variable
        # step_num2factor(factor_names,
        #             transform = function(x) x + 1,
        #             levels = Levels) %>%
    step_YeoJohnson(time_spend_company) %>% #this is the transformation step for my skewed variable
    step_normalize(all_numeric()) %>%
    step_nzv(all_predictors()) %>% #This will remove predictor variables with variance near zero
    step_dummy(all_nominal_predictors()) %>% #This will dummy code my nominal predictor variables
    step_adasyn(all_outcomes()) #This is the new adasyn step (skip = TRUE will make sure when the recipe is 'baked' that this adasyn will not apply to the test data)

adasyn_rec

```

All five recipes are built. Now I will create specifications for all the different models I want to fit and compare.
First, I will call the rules and baguette packages to my library.

```{r extra packages}
library(rules)
library(baguette)
```


#MODELS
I will create the specs for four different models: logistic regression, two tree-based models (random forest and boosted), and support vector machine (svm)


#Logistic Regression
```{r log spec}
logistic_reg_spec <- #specifying that the model is a logistic regression
    logistic_reg(penalty = tune(), mixture = tune()) %>% #penalty and mixture are set to tune, they will be determined later on when I fit the model. This will determine what regularization tactic (Lasso, Ridge, or Elastic-net) is used.
    set_engine("glmnet") %>% #using glmnet instead of glm per error message below when trying to set up the grid.
    set_mode("classification") #since my outcome variable is categorical, I will set this mode to classification.
```

# Random Forest
```{r forest spec}

rf_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger") %>%
  set_args(importance = "permutation") # Specify importance inside set_args()



```

#xgBoost
```{r boost spec}
xgb_spec <- boost_tree(tree_depth = tune(),
               learn_rate = tune(),
               loss_reduction = tune(),
               min_n = tune(),
               sample_size = tune(),
               trees = tune()) %>% #all are set to tune so fitting the model will help tune it
    set_engine("xgboost") %>% 
    set_mode("classification")
```

#svm
```{r svm, eval = FALSE}
#COULDN'T GET SVM TO WORK! Leaving this in to tinker with later on though
svm_spec <- svm_linear(
    cost = tune(),
    margin = tune()) %>%  
    set_engine("kernlab") %>% 
    set_mode("classification")
```


#WORKFLOW SETS
I will now start the process of creating my workflow set. I will create a list of all five recipes.
```{r rec list}
recipe_list <- list(
    NOSAMPLING = no_sample_rec,
    DOWNSAMPLE = downsample_rec,
    # SMOTE = smote_rec,
    ADASYN = adasyn_rec,
    UPSAMPLE = upsample_rec
)
```


And a list of all four model specs.
```{r model list}
model_list <- list(Logistic_Reg = logistic_reg_spec,
                   Random_Forest = rf_spec,
                   xgBoosted_Trees = xgb_spec
                   # SVM = svm_spec
                   )
```


And the list of all metrics I will want to use to examine the performance of my models later on. *I want to make sure this is a fully inclusive list, since I won't be able to look at other metrics not included on the list later on.
```{r metrics}
conflicts_prefer(yardstick::accuracy)
conflicts_prefer(yardstick::precision)

class_metric <- metric_set(accuracy, 
                           f_meas, 
                           j_index, 
                           kap, 
                           precision, 
                           sensitivity, 
                           specificity, 
                           roc_auc, 
                           mcc, 
                           pr_auc)

```

And here is my workflow set. It will include all five recipes, all four models, and the cross = TRUE argument will ensure that each recipe is applied to each model. I should have 20 total combinations.
```{r wf set}
wf_set <- workflow_set(
    preproc = recipe_list,
    models = model_list,
    cross = TRUE
)

wf_set
```

#RACING - EVALUATION
Now I can screen my models before I begin the process of tuning them
I will use the racing approach. This is very useful because as R is evaluating my different models, it will stop the evaluation process if it realizes that the model is not going to perform well.
```{r racing}
start_time <- now()

doParallel::registerDoParallel()

library(finetune)
# Load required packages
library(mlr3)
library(mlr3tuning)
library(mlr3pipelines)
library(data.table)
library(parallelMap)
library(mlr3measures)


race_ctrl <- control_race(
        save_pred = TRUE,
        parallel_over = "everything",
        save_workflow = TRUE)

race_results <- wf_set %>%
    workflow_map(
        "tune_race_anova",
        seed = 2023,
        resamples = cv_folds,
        grid = 50,
        metrics = class_metric,
        control = control_race()
    )

end_time <- now()

end_time - start_time

race_results
```

And my racing results are complete! I know it worked for all workflows because under results, it says tune_race for each one.

Now I can start the process of 'poking at' and analyzing my results. This should give me some basic information on the performance of the different combinations of model and recipe.

#PRECISION RECALL AREA UNDER THE CURVE
These are two visual representations of performance based on the metric of precision recall area under the curve (pr_auc). The first is a more basic autoplot, and the second gives information about what recipe (sampling method) and model specification (logistic, boosted trees, random forest) was used for each.
```{r pr-auc plot}
autoplot(
   race_results,
   rank_metric = "pr_auc",  
   metric = "pr_auc",       
   select_best = TRUE  
)

collect_metrics(race_results) %>% 
    separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
    filter(.metric == "pr_auc") %>% 
    group_by(wflow_id) %>% 
    filter(mean == max(mean)) %>% 
    group_by(model) %>% 
    select(-.config) %>% 
    distinct() %>%
    ungroup() %>% 
    mutate(Workflow_Rank =  row_number(-mean),
           .metric = str_to_upper(.metric)) %>%
    ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) + # shape = Recipe (cut off anything with "_" due to separate above)
    geom_point() +
    geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
    theme_minimal()+
    scale_colour_viridis_d() +
    labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "PR_AUC", color = "Model Types", shape = "Recipes")
```
One obvious trend is that logistic regression did not perform well regarding this metric. Boosted trees and random forest both performed much better, and quite similarly. The best overall was a boosted trees model using upsampling. However, all 8 random forest and boosted tree models were very close in performance here. 


#ROC AREA UNDER THE CURVE
Now I will create the same for roc area under the curve (roc_auc)
```{r roc-auc plot}
autoplot(
   race_results,
   rank_metric = "roc_auc",  
   metric = "roc_auc",       
   select_best = TRUE  
)


collect_metrics(race_results) %>% 
    separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
    filter(.metric == "roc_auc") %>% 
    group_by(wflow_id) %>% 
    filter(mean == max(mean)) %>% 
    group_by(model) %>% 
    select(-.config) %>% 
    distinct() %>%
    ungroup() %>% 
    mutate(Workflow_Rank =  row_number(-mean),
           .metric = str_to_upper(.metric)) %>%
    ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) + # shape = Recipe (cut off anything with "_" due to separate above)
    geom_point() +
    geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
    theme_minimal()+
    scale_colour_viridis_d() +
    labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "ROC_AUC", color = "Model Types", shape = "Recipes")
```
The results are very similar to that of pr_auc. The top performer, by a small margin, was the random forest with Adasyn.

#J-INDEX
```{r j-index plot}
autoplot(
   race_results,
   rank_metric = "j_index",  
   metric = "j_index",       
   select_best = TRUE  
)


collect_metrics(race_results) %>% 
    separate(wflow_id, into = c("Recipe", "Model_Type"), sep = "_", remove = F, extra = "merge") %>%
    filter(.metric == "j_index") %>% 
    group_by(wflow_id) %>% 
    filter(mean == max(mean)) %>% 
    group_by(model) %>% 
    select(-.config) %>% 
    distinct() %>%
    ungroup() %>% 
    mutate(Workflow_Rank =  row_number(-mean),
           .metric = str_to_upper(.metric)) %>%
    ggplot(aes(x=Workflow_Rank, y = mean, shape = Recipe, color = Model_Type)) + # shape = Recipe (cut off anything with "_" due to separate above)
    geom_point() +
    geom_errorbar(aes(ymin = mean-std_err, ymax = mean+std_err)) +
    theme_minimal()+
    scale_colour_viridis_d() +
    labs(title = "Performance Comparison of Workflow Sets", x = "Workflow Rank", y = "J-Index", color = "Model Types", shape = "Recipes")

```
And j-index was similar as well. Top performer was random forest with no sampling - interesting. Also, upsampling appeared twice in the plot, which I thought was peculiar. That did not occur with the two 'area under the curve' plots.


Overall, it seems that the metrics are favoring one of the tree-based models and not favoring any logistic specifications.

#FINALIZING MODELS
Now that I have a good idea of what is performing well and what is not, I will choose the top option from each model specification, update any parameters that require updating, fit to the training set, and save each one as an RDS file to make my life easier later on.


#RANDOM FOREST
For the top performer for random forest, I selected the model that used Adasyn sampling since it was the top performer for ROC and the second-highest for PR AUC.
```{r rf best model 1}
RF_best_results <-
    race_results %>%
    extract_workflow_set_result("ADASYN_Random_Forest") %>%
    select_best(metric = "pr_auc")


RF_best_results
```

I will be saving this model, but if I didn't want to for any reason, I could also copy down this info from the above output (mtry = 3, min_n = 12) and use it to tune the model.


```{r rf best model 2}
RF_test_results <-
    race_results %>%
    extract_workflow("ADASYN_Random_Forest") %>%
    finalize_workflow(RF_best_results) %>%
    last_fit(split = data_split) 
```


#RANDOM FOREST - VARIABLE IMPORTANCE
This will tell me which variables are the major players in the model. What are the strongest predictors of variance in attrition when using a random forest?
```{r rf var imp}
RF_params_best_model <- race_results %>%
    extract_workflow_set_result(id = "ADASYN_Random_Forest") %>%
    select_best(metric = "pr_auc")

RF_WF_fit_final <- race_results %>%
    extract_workflow("ADASYN_Random_Forest") %>%
    finalize_workflow(RF_params_best_model) %>%
    fit(Data)

### Important variables

RF_importance_tbl <- vip::vi(RF_WF_fit_final$fit$fit$fit)


RF_WF_fit_final %>%
    fit(Data) %>%
    extract_fit_parsnip() %>%
    vip::vi()



vip::vip(RF_WF_fit_final$fit$fit$fit,
         geom = "col",
         aesthetics = list(fill = "steelblue")) +
    labs(title = "Feature Importance")

```
It looks like satisfaction level is a major driver, as it time spent at the company. It also looks like about 5 variables are really 'levers to pull' regarding attrition (other three are number of projects, average monthly hours, andlast evaluation).

Saving the model - I will now save the model as an RDS file. This file type will essentially save this model as an object that can be shared and used by others without them having to run all the previous lines of code!
```{r rf save rds}
saveRDS(RF_WF_fit_final, file = "~/Documents/DirectoryR/UGA IOMP 2023 AA/RF_model_attrition_2023.rds")
```

#RANDOM FOREST - PREDICTIONS
Now I can use my fitted model to make predictions regarding attrition. What is the probability of an individual leaving the organization?
```{r rf predictions setup}
RF_predictions_tbl <- RF_WF_fit_final %>%
    predict(new_data = Data) %>%
    bind_cols(Data)


#Probabilities
RF_prob_predictions <- RF_WF_fit_final %>%
    predict(new_data = Data,
            type = "prob") 

RF_predictions_tbl <- RF_prob_predictions %>%
    bind_cols(RF_predictions_tbl)
```

```{r rf predictions collect}
RF_test_results %>%
    collect_predictions()
```

#RANDOM FOREST - PR CURVE
The PR curve shows the trade-off between precision and recall. I want a line that hugs the top right corner as much as possible.
```{r rf pr curve}

RF_test_results %>%
  collect_predictions() %>%
  pr_curve(left, .pred_0) %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_path() + 
  coord_equal() +
  theme_bw()

```
The curve is looking good.

# RANDOM FOREST - ROC CURVE
For the ROC curve shows the trade-off between sensitivity and specificity. This time, I want a curve that hugs the top left corner as much as possible.
```{r rf roc curve}
RF_test_results %>%
    collect_predictions() %>%
    roc_curve(left, .pred_0) %>%
    ggplot(aes(x = 1 - specificity, y = sensitivity)) +
    geom_line(size = 1.5, color = "midnightblue") +
    geom_abline(
        lty = 2, alpha = 0.5,
        color = "gray50",
        size = 1.2
    )
```
Once more, my visual representations of performance for the random forest are looking quite good so far.

# RANDOM FOREST - CONFUSION MATRIX
The confusion matrix is more cut and dry than the graphs above. It will give me the number of true and false negatives and positives in a 2x2 matrix.
```{r rf conf mat}
conflict_prefer("spec", "yardstick")


RF_test_results %>%
    collect_predictions() %>%
    conf_mat(truth = left, estimate = .pred_class) 

```

This look quite promising!
For those who don't leave (0), this is predicting 1365/1398 (97.6%) correctly.
For those who leave (1), this is predicting 281/314 (89.5%) correctly.


```{r rf metrics}
RF_test_results %>%
    collect_predictions() %>%
    conf_mat(truth = left, estimate = .pred_class) %>%
    summary()
```
And I can see that the summary matches what I calculated above.
Interestingly, since I have the same number of false positives and false negatives (33), my precision and recall are the same (0.976).

My j-index is 0.871. This is high, but if I could get it to be even higher, that would be great.

#RANDOM FOREST - SUMMARY
The results of the random forest are quite impressive. Its performance was promising, but I will see if the boosted trees model can perhaps increase the specificity to capture more individuals who will leave the organization. Since attrition is so expensive, I may accept a model that gives me more false predictions of attrition if it increases the true predictions of someone leaving. Also, as I mentioned above, having a model with a higher j-index would be preferrable as well. I will test my other models and compare all performance results.


#XGBOOST
For the top performer of my boosted trees models, I selected the model that used upsampling. It has the top performance for PR AUC and the second-highest for ROC. 
```{r xgb best model 1}
XGB_best_results <-
    race_results %>%
    extract_workflow_set_result("UPSAMPLE_xgBoosted_Trees") %>%
    select_best(metric = "pr_auc")


XGB_best_results
```
Once more, similar to the random forest, I could simply note these parameters and use them to tune if I didn't plan on saving the model.


```{r xgb best model 2}
XGB_test_results <-
    race_results %>%
    extract_workflow("UPSAMPLE_xgBoosted_Trees") %>%
    finalize_workflow(XGB_best_results) %>%
    last_fit(split = data_split) 
```


#XGB - VARIABLE IMPORTANCE
This will tell me which variables are the major players in the model. What are the strongest predictors of variance in attrition when using a boosted trees model?
```{r xgb var imp}
XGB_params_best_model <- race_results %>%
    extract_workflow_set_result(id = "UPSAMPLE_xgBoosted_Trees") %>%
    select_best(metric = "pr_auc")

XGB_WF_fit_final <- race_results %>%
    extract_workflow("UPSAMPLE_xgBoosted_Trees") %>%
    finalize_workflow(XGB_params_best_model) %>%
    fit(Data)

### Important variables

XGB_importance_tbl <- vip::vi(XGB_WF_fit_final$fit$fit$fit)


XGB_WF_fit_final %>%
    fit(Data) %>%
    extract_fit_parsnip() %>%
    vip::vi()



vip::vip(XGB_WF_fit_final$fit$fit$fit,
         geom = "col",
         aesthetics = list(fill = "steelblue")) +
    labs(title = "Feature Importance")

```
It looks like, once again, satisfaction level is important, as is time spent at the company. Interestingly, when compared to the random forest that had a higher quantity of important variables, there is a pretty major drop-off in importance after time spent at the company.

Last step, I know from my prior data poking that both satisfaction level and evaluation level would be related to attrition such that lower satisfaction/lower evaluation would be predictive of higher attrition. However, I don't know yet about the relationship of time at the company and attrition, so I will create a quick table here to see what the overall pattern is
```{r}
train_data %>%
    group_by(time_spend_company) %>% 
    summarise(avg_attrition = round(mean(as.numeric(left == '1')),2)) %>%
    arrange(desc(avg_attrition))

```
That's an interesting pattern - it does appear that the lowest attrition rates occur with those who have been with the company the longest (perhaps this could be considered a 'loyalty' type situation, where those who have put the most time in are the most committed to staying). But the highest rate of attrition is in the mid-range for time spent at the company. I would say that overall, the simplified version of the relationship is that as time spent at the company increases, attrition decreases, but with an interesting caveat that the mid-range employees may be most at risk for leaving. s

Saving the model - I will now save the model as an RDS file. This file type will essentially save this model as an object that can be shared and used by others without them having to run all the previous lines of code!
```{r xgb save rds}
saveRDS(XGB_WF_fit_final, file = "~/Documents/DirectoryR/UGA IOMP 2023 AA/XGB_model_attrition_2023.rds")
```

#XGB - PREDICTIONS
Now I can use my fitted model to make predictions regarding attrition. What is the probability of an individual leaving the organization?
```{r xgb predictions setup}
XGB_predictions_tbl <- XGB_WF_fit_final %>%
    predict(new_data = Data) %>%
    bind_cols(Data)


#Probabilities
XGB_prob_predictions <- XGB_WF_fit_final %>%
    predict(new_data = Data,
            type = "prob") 

XGB_predictions_tbl <- XGB_prob_predictions %>%
    bind_cols(XGB_predictions_tbl)
```

```{r xgb predictions table}
XGB_test_results %>%
    collect_predictions()
```

#XGB - PR CURVE
The PR curve shows the trade-off between precision and recall. I want a line that hugs the top right corner as much as possible.
```{r xgb pr curve}

XGB_test_results %>%
  collect_predictions() %>%
  pr_curve(left, .pred_0) %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_path() + 
  coord_equal() +
  theme_bw()

```
The curve is looking good. Quite similar to the random forest.

#XGB - ROC CURVE
For the ROC curve shows the trade-off between sensitivity and specificity. This time, I want a curve that hugs the top left corner as much as possible.
```{r xgb roc curve}
XGB_test_results %>%
    collect_predictions() %>%
    roc_curve(left, .pred_0) %>%
    ggplot(aes(x = 1 - specificity, y = sensitivity)) +
    geom_line(size = 1.5, color = "midnightblue") +
    geom_abline(
        lty = 2, alpha = 0.5,
        color = "gray50",
        size = 1.2
    )
```
This is looking promising as well.

#XGB - CONFUSION MATRIX
The confusion matrix is more cut and dry than the graphs above. It will give me the number of true and false negatives and positives in a 2x2 matrix.
```{r xgb conf mat}
conflict_prefer("spec", "yardstick")


XGB_test_results %>%
    collect_predictions() %>%
    conf_mat(truth = left, estimate = .pred_class) 

```
This is quite comparable to the random forest results.
Predicting 'no' (0) responses - 1381/1398 = 98.7% (slightly better than RF)
Predicting 'yes' (1) responses - 280/314 = 89.2% (slightly worse than RF)




```{r xgb metrics}
XGB_test_results %>%
    collect_predictions() %>%
    conf_mat(truth = left, estimate = .pred_class) %>%
    summary()
```
The boosted trees model also performed quite well. Comparable to the random forest model, the metrics are similar in several ways. Specificity was almost identical (0.892 here, 0.895 for RF), and precision was the same (0.976). The j-index was also similar (0.880 here, 0.871 in RF), although the j-index is improved here compared to the random forest. The other main difference in the boosted trees model is false positives: the number of individuals that wouldn't leave the organization but the model falsely predicts they would is less here than the random forest (17 here, 33 in RF).


#XGB - SUMMARY
The results of the boosted trees model are quite accurate, and they are also very comparable to the random forest model. As I mentioned above, the two main differences I see are:
Slightly better j-index for boosted trees.
Slightly lower count of false positives for boosted trees.

These small differences currently have me leaning toward the boosted trees model as the model with the best performance. I will withold final judgments until I have all my top model performance metrics (but I know that logistic regression will likely perform the worst based on the initial plots of all models).


#LOGISTIC REGRESSION (LR)
For the top performer out of my logistic regression models, the one that performed the best in all three metrics (PR, ROC, J-index) was strangely the one that used no sampling. So that is what I will go with. However, from my cursory glance of how far below the performance of the logistic models was compared to that of the tree-based models, I m not holding my breath that this will be my final model.
```{r lr best model}
LR_best_results <-
    race_results %>%
    extract_workflow_set_result("NOSAMPLING_Logistic_Reg") %>%
    select_best(metric = "pr_auc")


LR_best_results
```
And as with my other two models, I could simply note these parameters and use them to tune if I didn't plan on saving the model.


```{r lr best model contd}
LR_test_results <-
    race_results %>%
    extract_workflow("NOSAMPLING_Logistic_Reg") %>%
    finalize_workflow(LR_best_results) %>%
    last_fit(split = data_split) 
```


#LR - VARIABLE IMPORTANCE
This will tell me which variables are the major players in the model. What are the strongest predictors of variance in attrition when using a boosted trees model?
```{r lr var imp}
LR_params_best_model <- race_results %>%
    extract_workflow_set_result(id = "NOSAMPLING_Logistic_Reg") %>%
    select_best(metric = "pr_auc")

LR_WF_fit_final <- race_results %>%
    extract_workflow("NOSAMPLING_Logistic_Reg") %>%
    finalize_workflow(LR_params_best_model) %>%
    fit(Data)

### Important variables

LR_importance_tbl <- vip::vi(LR_WF_fit_final$fit$fit$fit)


LR_WF_fit_final %>%
    fit(Data) %>%
    extract_fit_parsnip() %>%
    vip::vi()



vip::vip(LR_WF_fit_final$fit$fit$fit,
         geom = "col",
         aesthetics = list(fill = "steelblue")) +
    labs(title = "Feature Importance")

```
This is interesting - very different results compared to my tree-based models. High salary and work accident are the strongest predictors of attrition in my logsitic model. Satisfaction level and time spent at company are only moderately important.

Saving the model - I will now save the model as an RDS file. This file type will essentially save this model as an object that can be shared and used by others without them having to run all the previous lines of code!
```{r lr save rds}
saveRDS(LR_WF_fit_final, file = "~/Documents/DirectoryR/UGA IOMP 2023 AA/LR_model_attrition_2023.rds")
```

#LR - PREDICTIONS
Now I can use my fitted model to make predictions regarding attrition. What is the probability of an individual leaving the organization?
```{r LR predictions setup}
LR_predictions_tbl <- LR_WF_fit_final %>%
    predict(new_data = Data) %>%
    bind_cols(Data)


#Probabilities
LR_prob_predictions <- LR_WF_fit_final %>%
    predict(new_data = Data,
            type = "prob") 

LR_predictions_tbl <- LR_prob_predictions %>%
    bind_cols(LR_predictions_tbl)
```

```{r lr predictions table}
LR_test_results %>%
    collect_predictions()
```

#LR - PR CURVE
The PR curve shows the trade-off between precision and recall. I want a line that hugs the top right corner as much as possible.
```{r lr pr curve}

LR_test_results %>%
  collect_predictions() %>%
  pr_curve(left, .pred_0) %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_path() + 
  coord_equal() +
  theme_bw()

```
The PR curve is not great. It isn't hugging the top right corner, especially compared to the curves from the two tree-based models.

#LR - ROC CURVE
For the ROC curve shows the trade-off between sensitivity and specificity. This time, I want a curve that hugs the top left corner as much as possible.
```{r lr roc curve}
LR_test_results %>%
    collect_predictions() %>%
    roc_curve(left, .pred_0) %>%
    ggplot(aes(x = 1 - specificity, y = sensitivity)) +
    geom_line(size = 1.5, color = "midnightblue") +
    geom_abline(
        lty = 2, alpha = 0.5,
        color = "gray50",
        size = 1.2
    )
```
And the same is true for the ROC curve. The hug to the top left corner is no longer present. I assumed this would be the case based on the cursory knowledge I had of the performance of logistic models.

#LR - CONFUSION MATRIX
The confusion matrix is more cut and dry than the graphs above. It will give me the number of true and false negatives and positives in a 2x2 matrix.
```{r lr conf mat}
conflict_prefer("spec", "yardstick")


LR_test_results %>%
    collect_predictions() %>%
    conf_mat(truth = left, estimate = .pred_class) 

```
And it seems that the primary problem with the logistic model is the number of false negatives - ie, the model is predicting that a high number of people will not leave when they, indeed, do leave the organization. This is highly problematic.
Overall, the model performs ok predicting who will stay, but we know that the more expensive error is missing who will leave.
Stats: 
Predicting 'no' (0) responses - 1377/1398 = 98.5% (this isn't bad)
Predicting 'yes' (1) responses - 53/314 = 16.9% (this is VERY bad)

Let's look at the metrics.

```{r lr metrics}
LR_test_results %>%
    collect_predictions() %>%
    conf_mat(truth = left, estimate = .pred_class) %>%
    summary()
```
The logistic model performs far more poorly than the two tree-based models. Recall is high, but overall accuracy is only 0.835, specificity (as mentioned above) is 0.169, and the j-index is 0.154. 
This is a major problem. If the model is flagging individuals as not likely to leave and they do leave, this is much more costly than flagging individuals as likely to leave when they were more likely to stay.


#LR - SUMMARY
Overall, the logistic model as it stands now is not a likely contender for the final model. I will continue to run analyses on it, however, because right now the threshold is set to 0.50 since that's the standard threshold (in a nutshell, below 0.5 and it predicts a 0, not leaving; above 0.5 and it predicts a 1, leaving). If the threshold is tuned more, there's a possibility that this model's performance could improve drastically. As it stands now, the ability of the model to accurately predict who leaves is far below the standard (and, worth noting, far below the performance of the previous two models).
The good news is that the next step is threshold analysis, which will help me learn more about the potential for the logistic model to improve from its current state.


#THRESHOLD ANALYSIS
As I stated above, the default threshold for all the models is 0.50. This could be far from the optimal threshold to set, especially since the cost of someone leaving is much greater than the cost of giving an intervention to someone who didn't intend to leave. Because of this, the next logical step is to run a threshold analysis to determine what threshold would be most optimal for my three models.

#THRESHOLD VISUALIZATION - RF
```{r rf threshold setup}
RF_params_best_model <- race_results %>%
    extract_workflow_set_result(id = "ADASYN_Random_Forest") %>%
    select_best(metric = "j_index") 

RF_WF_fit <- race_results %>%
    extract_workflow("ADASYN_Random_Forest") %>%
    finalize_workflow(RF_params_best_model) %>%
    fit(training(data_split))
```

```{r rf threshold visual}
RF_WF_fit %>% 
  predict(new_data = testing(data_split), type = 'prob') %>% 
  bind_cols(testing(data_split)) %>% 
  ggplot(aes(x=.pred_1, fill = left, color = left)) +
    geom_histogram(bins = 40, alpha = 0.5) +
    theme_minimal() +
    scale_fill_viridis_d(aesthetics = c('color', 'fill'), end = 0.8) +
    labs(title = 'Distribution of Prediction Probabilities by Attrition Status - Random Forest', x = 'Probability Prediction', y = 'Count')
```

#THRESHOLD VISUALIZATION - XGB
```{r xgb threshold setup}
XGB_params_best_model <- race_results %>%
    extract_workflow_set_result(id = "UPSAMPLE_xgBoosted_Trees") %>%
    select_best(metric = "j_index") 

XGB_WF_fit <- race_results %>%
    extract_workflow("UPSAMPLE_xgBoosted_Trees") %>%
    finalize_workflow(XGB_params_best_model) %>%
    fit(training(data_split))
```

```{r xgb threshold visual}
XGB_WF_fit %>% 
  predict(new_data = testing(data_split), type = 'prob') %>% 
  bind_cols(testing(data_split)) %>% 
  ggplot(aes(x=.pred_1, fill = left, color = left)) +
    geom_histogram(bins = 40, alpha = 0.5) +
    theme_minimal() +
    scale_fill_viridis_d(aesthetics = c('color', 'fill'), end = 0.8) +
    labs(title = 'Distribution of Prediction Probabilities by Attrition Status - Boosted Trees', x = 'Probability Prediction', y = 'Count')
```


#THRESHOLD VISUALIZATION - LOGISTIC
```{r lr threshold setup}
LR_params_best_model <- race_results %>%
    extract_workflow_set_result(id = "NOSAMPLING_Logistic_Reg") %>%
    select_best(metric = "j_index") 

LR_WF_fit <- race_results %>%
    extract_workflow("NOSAMPLING_Logistic_Reg") %>%
    finalize_workflow(LR_params_best_model) %>%
    fit(training(data_split))
```

```{r lr threshold visual}
LR_WF_fit %>% 
  predict(new_data = testing(data_split), type = 'prob') %>% 
  bind_cols(testing(data_split)) %>% 
  ggplot(aes(x=.pred_1, fill = left, color = left)) +
    geom_histogram(bins = 40, alpha = 0.5) +
    theme_minimal() +
    scale_fill_viridis_d(aesthetics = c('color', 'fill'), end = 0.8) +
    labs(title = 'Distribution of Prediction Probabilities by Attrition Status - Logistic Regression', x = 'Probability Prediction', y = 'Count')
```

#THRESHOLD ANALYSIS - COMPARING VISUALS
The visuals were quite useful. My first insight from them is how well the two tree-based models do in accurately predicting attrition compared to logistic regression. Based on the graph for logistic regression, I believe the model would perform better with a higher threshold.

#THRESHOLD EVALUATION - LOGISTIC
After some trial and error, I figured out that the threshold_perf() function requires the probably package to be called.
```{r lr threshold eval}
#Generate Probability Prediction Dataset
library(probably)
LR_WF_pred <- LR_WF_fit %>% 
  predict(new_data = testing(data_split), type = 'prob') %>% 
  bind_cols(testing(data_split)) %>% 
  select(left, .pred_0, .pred_1)
```


```{r lr threshold tibble}
#Generate Sequential Threshold Tibble
threshold_data_LR <- LR_WF_pred %>% 
threshold_perf(truth = left, estimate = .pred_0, thresholds = seq(0.1, 1, by = 0.01))
```

```{r lr threshold max j setup}
#Identify Threshold for Maximum J-Index
max_j_index <- threshold_data_LR %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate)) %>% 
  select(.threshold) %>% 
  as_vector()
```

```{r lr threshold max j}
max_j_index_with_estimate <- threshold_data_LR %>%
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate)) %>% 
  select(.threshold, .estimate) %>% 
  as_tibble
```



```{r lr threshold graph}
threshold_data_LR %>% 
  filter(.metric != 'distance') %>% 
  ggplot(aes(x=.threshold, y=.estimate, color = .metric)) +
   geom_line(size = 2) +
   geom_vline(xintercept = max_j_index, lty = 5, alpha = .6) +
   theme_minimal() +
   scale_colour_viridis_d(end = 0.8) +
   labs(x='Threshold', 
        y='Estimate', 
        title = 'Balancing Performance by Varying Threshold',
        subtitle = 'Vertical Line = Max J-Index',
        color = 'Metric')
```


```{r lr threshold jindex}
max_j_index_with_estimate

```

It turns out, a threshold of 0.86 (significantly higher than the default 0.50) would increase our j-index to 0.527. This is still much, much lower than the j-index, and overall performance, of either of my tree-based models.

I will now repeat this process with my random forest.

#THRESHOLD EVALUATION - RANDOM FOREST
After some trial and error, I figured out that the threshold_perf() function requires the probably package to be called.
```{r rf threshold eval}

RF_WF_pred <- RF_WF_fit %>% 
  predict(new_data = testing(data_split), type = 'prob') %>% 
  bind_cols(testing(data_split)) %>% 
  select(left, .pred_0, .pred_1)
```


```{r rf threshold tibble}
threshold_data_RF <- RF_WF_pred %>% 
threshold_perf(truth = left, estimate = .pred_0, thresholds = seq(0.1, 1, by = 0.01))
```

```{r rf threshold max j setup}
#Identify Threshold for Maximum J-Index
max_j_index <- threshold_data_RF %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate)) %>% 
  select(.threshold) %>% 
  as_vector()
```

```{r rf threshold max j}
max_j_index_with_estimate <- threshold_data_RF %>%
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate)) %>% 
  select(.threshold, .estimate) %>% 
  as_tibble
```


```{r rf threshold graph}
threshold_data_RF %>% 
  filter(.metric != 'distance') %>% 
  ggplot(aes(x=.threshold, y=.estimate, color = .metric)) +
   geom_line(size = 2) +
   geom_vline(xintercept = max_j_index, lty = 5, alpha = .6) +
   theme_minimal() +
   scale_colour_viridis_d(end = 0.8) +
   labs(x='Threshold', 
        y='Estimate', 
        title = 'Balancing Performance by Varying Threshold - RF',
        subtitle = 'Vertical Line = Max J-Index',
        color = 'Metric')
```


```{r rf threshold jindex}
max_j_index_with_estimate

```
And this tells me that if I adjust the threshold to 0.56, my random forest has a j-index of 0.882. This is definitely an improvement. However, I have a feeling that the boosted trees model may still outperform my other two models. I will go through this process one more time.


#THRESHOLD EVALUATION - BOOSTED TREE
```{r xgb threshold eval}

XGB_WF_pred <- XGB_WF_fit %>% 
  predict(new_data = testing(data_split), type = 'prob') %>% 
  bind_cols(testing(data_split)) %>% 
  select(left, .pred_0, .pred_1)
```


```{r xgb threshold tibble}
threshold_data_XGB <- XGB_WF_pred %>% 
threshold_perf(truth = left, estimate = .pred_0, thresholds = seq(0.1, 1, by = 0.01))
```

```{r xgb threshold max j setup}
max_j_index <- threshold_data_XGB %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate)) %>% 
  select(.threshold) %>% 
  as_vector()
```

```{r xgb threshold max j}
max_j_index_with_estimate <- threshold_data_XGB %>%
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate)) %>% 
  select(.threshold, .estimate) %>% 
  as_tibble
```


```{r xgb threshold graph}
threshold_data_XGB %>% 
  filter(.metric != 'distance') %>% 
  ggplot(aes(x=.threshold, y=.estimate, color = .metric)) +
   geom_line(size = 2) +
   geom_vline(xintercept = max_j_index, lty = 5, alpha = .6) +
   theme_minimal() +
   scale_colour_viridis_d(end = 0.8) +
   labs(x='Threshold', 
        y='Estimate', 
        title = 'Balancing Performance by Varying Threshold - XGB',
        subtitle = 'Vertical Line = Max J-Index',
        color = 'Metric')
```


```{r xgb threshold jindex}
max_j_index_with_estimate

```

And if the threshold is 0.53 for my boosted trees model, the j-index will be 0.885. This is a slight improvement above the default threshold for boosted trees (0.880) and above the random forest with adjusted threshold (0.882).

So now it's time to make my ultimate decision regarding what model I want to proceed with as my final model.

#FINAL MODEL - DECISION
I have a comprehensive view of my three potential models: logistic regression with no sampling, random forest with Adasyn, and boosted tree model with upsampling. I will discuss the performance of the three models based on the following five criteria, then make a choice regarding my final model:
o	Confusion Matrix Performance
o	AUC performance
o	J-index
o	Simplicity
o	Interpretability 

#CONFUSION MATRIX PERFORMANCE
The confusion matrices of the random forest and boosted tree model were quite encouraging, and they were also very similar.
The random forest correctly predicted who didn't leave the organization 97.6% of of the time (1365/1398) and it correctly predicted who did leave the organization 89.5% of the time (281/314).
The boosted tree model correctly predicted who didn't leave the organization 98.7% of the time (1381/1398) and it correctly predicted who did leave the organization 89.2% of the time (280/314).
The results are very comparable, and although the random forest had slightly better sensitivity, the boosted tree model had slightly better specificity. At this point, I am treating their performance in the confusion matrix as virtually equal.
The logistic model, however, did not have an impressive confusion matrix performance. It correctly predicted who didn't leave the organization 98.5% of the time (1377/1398). However, its ability to correctly predict who did leave the organization was quite poor at only 16.9% (53/314).

My conclusion from the confusion matrices is that the random forest and boosted trees model are equally top contenders, and the logistic regression is not looking good.

#AUC PERFORMANCE
For this, I am looking at the ROC, as well as the PR curves of my three models. For the ROC curve, I am looking for the line to hug the top left corner as much as possible, indicating a high percentage of area under the curve and high levels of both specificity (low numbers of false negatives) and sensitivity (low numbers of false positives). For the PR curve, I am looking for the line to hug the top right corner as much as possible, indicating high levels of both precision (accuracy in predicting who leaves) and recall (completeness in capturing who leaves).
Similar to the confusion matrix results, both the random forest and boosted tree model performed comparably well. When I ran the initial comparison of all 12 potential models, the top performer for the PR AUC was the upsample boosted tree model, and the second-best performer was the Adasyn random forest model. The top performer for the ROC AUC was the Adasyn random forest model, and the second-best performer was the upsample boosted tree model.
These results were confirmed by the graphing the curves for the two models later on. Both random forest and boosted trees performed equally well regarding area under the curve.
The logistic model did not perform well in AUC. The logistic model was outperformed by all random forest and boosted tree model variants. It did perform better than the other logistic model variants, but overall it is still consistently the 'bottom of the pack' when compared to the random forest and boosted tree model. This was confirmed by graphing the curves later. Both curves did not hug the corners, especially when compared to the visual representations of the curves for both random forest and boosted tree models.

My conclusion from examination of the AUC is that random forest and boosted trees are continuing to 'tie for first place' and the logistic model is continuing to perform less well compared to the other two models.

#J-INDEX
J-index is a metric calculated from the ROC curve. The closer a model's j-index is to 1, the better. I received initial insights about the j-index for each of my top models from the workflow performance comparison. My boosted tree model slightly outperformed my random forest, but they were very close. My logistic model was, once more, the worst performer by a significant margin.
This was confirmed in my collection of prediction metrics for each of the three models:
-Random Forest, j-index = 0.871
-Boosted Tree, j-index = 0.880
-Logistic, j-index = 0.154
Boosted tree performs slightly better than random forest, but they are very close. Boosted tree may be pulling slightly ahead at this point.
Logistic's j-index is very poor. I am officially taking logistic regression out of the running, so for my next two criteria, I will be trying to narrow it down from my two tree-based models.

#SIMPLICITY
I will now evaluate my two tree-based models for simplicity. I am looking for the model that can explain a high percentage of the variance in attrition with as few variables as possible. So for this, I will look back to my analyses for variable importance. The results of these analyses gave me VIPs (variable importance plots) with a table of the variable importance numbers as well.
I have set a threshold of 0.10. Meaning I will look at the predictor variables that have a variable importance score of 0.10 or higher for my two models.
First I will look at my random forest. The results indicate that the following five predictors have the highest variable importance scores:
Satisfaction Level = 0.24234847			
Time Spent at Company = 0.22564894			
Number of Projects = 0.18585139			
Average Monthly Hours Worked = 0.14453009			
Last Evaluation = 0.13972133

And for my boosted trees model, the results indicate that the following three predictors have the highest variable importance scores:
Satisfaction Level = 0.442646573			
Time Spent at Company = 0.300437010			
Last Evaluation = 0.119731924

Interestingly (and perhaps unsurprisingly) the top two variables for both models were the same: Satisfaction Level and Time Spent at Company. I have noted the following:
In general, the VI scores for the boosted tree model (0.12 - 0.44) are higher than those for the random forest (0.14 - 0.24). Also, the boosted tree model has a smaller number of important variables than the random forest model.
So I can explain the variance in attrition with a more parsimonious set of variables with my boosted tree model.

Also, this may be slightly outside the purview of 'simplicity' - but random forest models are sometimes more prone to overfitting than boosted tree models. Just another point in favor of the boosted tree.
The random forest is still a decent contender, but this definitely helps the boosted tree model pull further ahead for me.

#INTERPRETABILITY
Lastly, I will examine the models based on their interpretability. How well can I explain to someone what is happening in the models? How well and easily can I adapt these explanations to fit different audience needs (other analytical/data-focused individuals, and business leaders, executives, or key stakeholders who want the less data-driven explanations and information)?
Both models are tree-based, which tend to be easier to explain and understand than some other models. Essentially, both models are based on decision trees for a classification outcome variable - these are like flowcharts in which a predictor variable would branch off into two groups or 'leaves' based on the models prediction of how likely each of the two outcomes is. The order of the predictor variables is usually based on some type of variable importance factor.
Tree-based models tend to be high interpretable because they are easily visualized (we've all seen a flowchart in some capacity) and easy to follow what the visualization means and how to understand the results/predictions. 
One decision tree is usually not robust to factors like complex models, outliers, or overfitting, which is where boosting, bagging, and/or random forests can come into play. To be brief, these are different methods of building an analyzing several decision trees that are all fed into the same model. For my random forest, I selected 1000 trees, and for my boosted tree model, I selected the tuning function which selected 292 trees.

The main difference between a random forest and a boosted tree model is that the trees in a random forest are all randomly sampled, and the trees in a boosted model are able to learn from the 'mistakes' of previous trees to hone in on a more accurate model with less reps (smaller number of trees). There are other more nuanced differences, but in layman's terms, this is the big one.
I think that, regarding interpretability, both of my tree-based models can be easily visualized, explained, and understood by both data and non-data people. However, the boosted tree model has a few advantages - one is that, with less trees, it may be an easier sell. And two, perhaps more importantly, is that the model is able to build on and learn from previous trees to hone in on a more accurate model with less trees. Because of this, my vote is for the boosted tree model regarding interpretability, but both are valid contenders here.

#FINAL MODEL - CONCLUSION
My decision for my final model is my boosted tree model. This model has the best sensitivity according to the confusion matrix, it was a top performer in both PR and ROC curve analyses, it had the highest j-index of my final three models, and the big differentiating factor between the boosted tree and random forest was the simplicity. Being able to explain what leads to attrition with only three variables instead of five is definitely advantageous. Lastly, although both tree-based models are highly interpretable, the boosted model involves less reps and potentially improved accuracy by learning from previous reps.

MY FINAL MODEL IS THE BOOSTED TREE MODEL WITH UPSAMPLING 

#RDS OF FINAL MODEL
I had previously saved my top three models, but just to finalize the code here, this is my RDS file for my final model.
```{r}
saveRDS(XGB_WF_fit_final, file = "~/Documents/DirectoryR/UGA IOMP 2023 AA/XGB_model_attrition_2023.rds")
```

#FINAL MODEL DESCRIPTION
My final model is my boosted tree model that utilizes upsampling. Tree-based models, as I mentioned above, are easly to interpret and understand. My boosted tree model was tuned to contain 292 trees, and the upsampling technique facilitates in correcting imbalanced datasets. Because the dataset countained significantly more 'no' (0) responses to attrition compared to the 'yes' (1) responses, (81.7% and 18.3% respectively), a model that did not account for this imbalance would be highly likely to be lazy and lean too heavily on predicting 'no' responses. Upsampling helps counteract this imbalance by creating more instances of the rarer response (in this case, the 'yes' responses).
In this model, the top three predictors of an individual leaving the organization were Satisfaction Level (from 0-1), Time Spent at the Company (from 2-10), and Last Evaluation (from 0-1). As I discussed during feature importance for this model, lower rates for satisfaction level and evaluation are associated with higher rates of attrition. And in general, lower values for time spent at the company are associated with higher attrition - however, the mid-range employees (5-6) have the highest rates of attrition. I will note this more in depth in my key stakeholder write-up.
This model provides a parsimonious look at predicting attrition by boiling the relationship down to three predictors: if employees are satisfied with their jobs (sat level), performing well and receiving positive evaluations (last eval), and have enough time under their belts with the organization (time spent), they are more likely to stay with the company.
A more layman's/business-focused description of the model will be included in the stakeholder write-up.

#COST FUNCTION
I will now run a cost function on my final model. I previously calculated the threshold that gives me the highest j-index for the boosted tree model, but what about the threshold that leads to the most cost-effective decision for the organization? In order to find this threshold, I will need to define the costs of each 'square' in my confusion matrix. Here are a few stats to factor in:
The cost of one individual leaving the organization is $100,000
The cost of the intervention per person is $5,000
The intervention success rate is 50%

For the following descriptions of the results I've calculated below, a 'positive' is associated with a 0 (not leaving) and a 'negative' is associated with a 1 (leaving). 

Here are the results I've calculated:
True Negative (TN): When the model accurately predicts that someone will leave, it will save the company $95,000 50% of the time (intervention occurs and attrition does not occur) and cost the company $105,000 the other 50% of the time (intervention occurs and attrition occurs).

True Positive (TP): When the model accurately predicts that someone will not leave, it always results in a net cost of $0 since no intervention occurs and no attrition occurs.

False Negative (FN): When the model inaccurately identifies someone as likely to leave and they are given an unnecessary intervention, it always costs the organization $5,000 (intervention occurs and attrition does not occur).

False Positive (FP): When the model inaccurately identifies someone as not going to leave, it always costs the organization $100,000 (no intervention occurs and attrition occurs).

There was no way to cleanly weave in the 'intervention only works 50% of the time' so I will move forward with my calculations as though the intervention works 100% of the time, and I will make some adjustments to counter this fact, document the crap out of it, and discuss it further in my stakeholder report.


For starters, I will calculate the costs of false positives and false negatives given the information above and the previous metrics collected for my model.
```{r threshold cost}
threshold_data_XGB %>% 
        filter(.metric %in% c('sensitivity', 'specificity')) %>% 
        pivot_wider(id_cols = .threshold, values_from = .estimate, names_from = .metric) %>% 
        mutate(Cost_FN = ((1-sensitivity) * 5000), 
               Cost_FP = ((1-specificity) * 100000),
               Total_Cost = Cost_FN + Cost_FP) %>%
        arrange(Total_Cost)
```

And I will visualize the results as well.
```{r threshold cost graph}
    threshold_data_XGB %>% 
        filter(.metric %in% c('sensitivity', 'specificity')) %>% 
        pivot_wider(id_cols = .threshold, values_from = .estimate, names_from = .metric) %>%
        mutate(Cost_FN = ((1-sensitivity) * 5000), 
               Cost_FP = ((1-specificity) * 100000),
               Total_Cost = Cost_FN + Cost_FP) %>% 
        select(.threshold, Cost_FN, Cost_FP, Total_Cost) %>% 
        pivot_longer(2:4, names_to = 'Cost_Function', values_to = 'Cost') %>% 
        ggplot(aes(x = .threshold, y = Cost, color = Cost_Function)) +
        geom_line(size = 1.5) +
        theme_minimal() +
        scale_colour_viridis_d(end = 0.8) +
        labs(title = 'Threshold Cost Function', x = 'Threshold')
 
```

And this is telling me to set my threshold at 0.99 to achieve the lowest cost. Although this makes sense given the low cost of the intervention compared to the cost of someone leaving, and the number of false negatives and false positives in my model, but it is not the most helpful result either. I will still calculate what the corresponding j-index would be for a 0.99 threshold.
```{r low cost jindex}
best_j_index_with_estimate_for_cost <- threshold_data_XGB %>%
  filter(.metric == 'j_index') %>% 
  filter(.threshold == 0.99) %>%
  select(.threshold, .estimate) %>% 
  as_tibble

best_j_index_with_estimate_for_cost
```
And the corresponding j-index is 0.607. Not great, especially compared to my previous threshold analysis for this model.

I have decided to find a 'Goldilocks' number by examining the dataframe above for cost analysis and seeing if a lower threshold will still provide a comparable cost function (and hopefully improve the j-index).
Based on my re-examination of the dataframe, I will also run a j-index calculation for a threshold of 0.80. I chose 0.80 because it is roughly between my highest j-index threshold (0.53) and lowest cost threshold (0.99) while still maintaining a relatively low total cost that toes the line between the total cost for 0.99 (4690.28) and for 0.53 (10255.46). The total cost was 8491.27 for a threshold of 0.80. I will see what the corresponding j-index is
```{r goldilock jindex}
best_j_index_with_estimate_for_cost <- threshold_data_XGB %>%
  filter(.metric == 'j_index') %>% 
  filter(.threshold == 0.80) %>%
  select(.threshold, .estimate) %>% 
  as_tibble

best_j_index_with_estimate_for_cost
```

This feels like a good middle ground to offer a lower cost solution that doesn't say 'just give the intervention to virtually everyone.'

I will now graph all of these results
```{r thresholds visual}
threshold_data_XGB %>% 
  filter(.metric %in% c('sensitivity', 'specificity')) %>% 
  pivot_wider(id_cols = .threshold, values_from = .estimate, names_from = .metric) %>% 
  mutate(Cost = ((1-sensitivity) * 5000) + ((1-specificity) * 100000),
         j_index = (sensitivity+specificity)-1) %>% 
  ggplot(aes(y=Cost, x = .threshold)) +
    geom_line() +
    geom_point(aes(size = j_index, color = j_index)) +
    geom_vline(xintercept = 0.53, lty = 2) +
    annotate(x = 0.41, y=13500, geom = 'text', label = 'Best Class Differentiation\nJ-Index = 0.89,\nCost = $10,255.46\nThreshold = 0.53') +
    geom_vline(xintercept = 0.99, lty = 2) +
    annotate(x = 0.93, y = 12500, geom = 'text', label = 'Lowest Cost Model\nJ-Index = 0.61,\nCost = $4,690.28\nThreshold = 0.99') +
    geom_vline(xintercept = 0.80, lty = 2) +
    annotate(x = 0.70, y = 7000, geom = 'text', label = 'Goldilocks Threshold\nJ-Index = 0.87,\nCost = $8,491.27\nThreshold = 0.80') +    
    theme_minimal() +
    scale_colour_viridis_c() +
    labs(title = 'Decision Threshold Attrition Cost Function', 
         subtitle = 'Where Cost(FN) = $5,000',
         x = 'Classification Threshold', size = 'J-Index', color = 'J-Index')
```

This graph shows the three potential thresholds to select: one that provides the highest j-index and would provide the best class differentiation between 'stayers' and 'leavers' - one that would offer the lowest cost (at the expense of giving virtually all people the intervention) - and a Goldilocks threshold that provides a compromise of both j-index and cost.


#Return on Investment (ROI)
I have one final component to my analysis! It is evaluating my model for its ROI, or return on investment. Is it good enough at predicting attrition that it is saving the company money? Based on what I've gathered from my analyses thus far, I believe it is. But I will calculate the specifics below based on the previous confusion matrix results.

I do need to bear a few things in mind when examining this:
One, this is collected using the 0.50 default threshold instead of the other options discussed and graphed above.
Two, I will be providing two alternate responses to the overall cost of the true negatives (accuractely predicting those who leave) - one will represent the cost function that each true negative saves the company $95,000, and one that takes into account the 50% success rate of the intervention.

As a reminder to myself, here's what I'm working with:
- TP = $0
- TN = +$95,000 (50%) and -$105,000 (50%)
- FP = -$100,000
- FN = $5,000

*And here is the information set up in my makeshift confusion matrix, just so I have the numbers already in place:
           Truth
Prediction    0         1
         0   $0         -$100,000
         1   -$5,000   +$95,000 / -$105,000
         
And here is the code to create a confusion matrix for my entire dataset using my boosted tree model.
```{r full conf matrix}
XGB_predictions_tbl %>%
    conf_mat(truth = left, estimate = .pred_class)
```

And here are the resulting values:
- TP = 7601*0 = 0
- TN (INT WORKS) = (2329/2)*95,000 = 110,627,500
- TN (INT FAILS) = (2329/2)*(-105,000) = -122,272,500
- FP = 35*(-100,000) = -3,500,000
- FN = -170,000

TOTAL = -15,315,000

This means that, when using the boosted tree model for predicting attrition and implementing an intervention that works 50% of the time, approximately 1200 employees will still leave the organization and, this would cost the organization (from both attrition and intervention) about $15,315,000.

However, without the predictive model and intervention, a total of 2364 employees would leave the organization and cost the organization (due to attrition alone) $236,400,000.

This is an incredibly impressive difference of $221,085,000! The ROI for this final model is over 200 million dollars, which means that this predictive model (and subsequent intervention) would have a substantial impact on employee attrition rates and has the potential to save the company hundreds of millions of dollars.




```{r}
{r ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}
```


